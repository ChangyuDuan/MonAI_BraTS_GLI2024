import os
import sys
import argparse
import torch
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from DatasetLoader_transforms import BraTSDatasetLoader
from model import SimpleBraTSModel, get_all_supported_models
from train import BraTSTrainer
from evaluate import BraTSEvaluator


def get_high_performance_config(device='auto') -> Dict[str, Any]:
    """
    è·å–é…ç½®æ€§èƒ½ï¼ˆé’ˆå¯¹CPUå’ŒGPUè®¾å¤‡ï¼‰
    é»˜è®¤ç»Ÿä¸€ä½¿ç”¨è‡ªé€‚åº”æŸå¤±å‡½æ•°ç­–ç•¥å’Œå®Œæ•´è¯„ä¼°æŒ‡æ ‡
    
    Args:
        device: è®¾å¤‡ç±»å‹ ('cpu', 'cuda', 'auto')
        
    Returns:
        Dict[str, Any]: é…ç½®å­—å…¸
    """
    # è®¾å¤‡æ€§èƒ½é…ç½®
    if device == 'cpu':
        # CPUæ€§èƒ½é…ç½®
        #batch_size = 4
        #cache_rate = 0.5
        # CPUæ€§èƒ½é…ç½®ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
        batch_size = 1  # é™ä½æ‰¹æ¬¡å¤§å°å‡å°‘å†…å­˜ä½¿ç”¨
        cache_rate = 0.1  # å¤§å¹…é™ä½ç¼“å­˜ç‡é¿å…å†…å­˜ä¸è¶³
        num_workers = 0  # Windowsä¸Šè®¾ç½®ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
        pin_memory = False
        spatial_size = (96, 96, 96)
        roi_size = (64, 64, 64)
        use_amp = False
    else:  # cuda or auto
        # GPUæ€§èƒ½é…ç½®
        batch_size = 8
        cache_rate = 1.0
        num_workers = 16
        pin_memory = True
        spatial_size = (128, 128, 128)
        roi_size = (96, 96, 96)
        use_amp = True
    
    # åŸºç¡€å­¦ä¹ ç‡
    learning_rate = 2e-4
    
    return {
        # æ•°æ®åŠ è½½é…ç½®
        'batch_size': batch_size,
        'cache_rate': cache_rate,
        'num_workers': num_workers,
        'pin_memory': pin_memory,
        'persistent_workers': True,
        'prefetch_factor': 4,
        
        # æ•°æ®é¢„å¤„ç†é…ç½®
        'spatial_size': spatial_size,
        'roi_size': roi_size,
        
        # è®­ç»ƒé…ç½®
        'max_epochs': 500,
        'learning_rate': learning_rate,
        'weight_decay': 1e-5,
        'use_amp': use_amp,
        'patience': 50,
        'save_interval': 10,
        'log_interval': 5,
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        'optimizer_name': 'AdamW',
        'scheduler_name': 'CosineAnnealingLR',
        
        # éªŒè¯é…ç½®
        'val_interval': 1,
        'val_split': 0.2,
        
        # ç»Ÿä¸€ç­–ç•¥é…ç½®ï¼ˆæ‰€æœ‰æ¨¡å‹ç±»å‹éƒ½ä½¿ç”¨ï¼‰
        'use_adaptive_loss': True,          # å¼ºåˆ¶ä½¿ç”¨è‡ªé€‚åº”æŸå¤±å‡½æ•°
        'use_full_metrics': True,           # å¼ºåˆ¶ä½¿ç”¨å®Œæ•´è¯„ä¼°æŒ‡æ ‡
        'loss_strategy': 'adaptive_combined', # æŸå¤±å‡½æ•°ç­–ç•¥
        'metrics_strategy': 'comprehensive', # è¯„ä¼°æŒ‡æ ‡ç­–ç•¥
        
        # å…¶ä»–é…ç½®
        'seed': 42,
        'gradient_clip_val': 1.0,
        'compile_model': True,  # æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
        'deterministic': True
    }

def merge_args_with_config(args, device) -> Dict[str, Any]:
    """
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        Dict[str, Any]: åˆå¹¶åçš„é…ç½®å­—å…¸
    """
# è·å–åŸºç¡€é…ç½®
    config = get_high_performance_config(
        device=str(device)
    )
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é»˜è®¤é…ç½®
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.epochs:
        config['max_epochs'] = args.epochs
    if args.learning_rate:
        config['learning_rate'] = args.learning_rate
        
    # æ·»åŠ å…¶ä»–å¿…è¦å‚æ•°
    config['data_dir'] = args.data_dir
    config['output_dir'] = args.output_dir
    config['device'] = str(device)
    
    return config

def run_simplified_training(args, device: torch.device):
    """
    è¿è¡Œè®­ç»ƒæµç¨‹ï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼ï¼š
    1. å¤šæ¨¡å‹è®­ç»ƒï¼šåˆ†åˆ«è®­ç»ƒæŒ‡å®šçš„å¤šä¸ªæ¨¡å‹ï¼Œå¯é€‰æ‹©åˆ›å»ºé›†æˆ
    2. å•æ¨¡å‹è®­ç»ƒï¼šè®­ç»ƒå•ä¸ªæŒ‡å®šæ¨¡å‹
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        device: è®¡ç®—è®¾å¤‡
    """
    print("\n" + "=" * 60)
    
    print("å¼€å§‹è®­ç»ƒæµç¨‹")
    
    # å¦‚æœå¯ç”¨é›†æˆä¸”æ¨¡å‹æ•°é‡è¾ƒå¤šï¼Œæç¤ºè¿™æ˜¯é›†æˆè®­ç»ƒ
    if args.enable_ensemble and len(args.model_names) > 3:
        print("æ£€æµ‹åˆ°å¤šæ¨¡å‹é›†æˆè®­ç»ƒæ¨¡å¼")
    
    print("=" * 60)
    
    try:
        # è‡ªåŠ¨è°ƒèŠ‚å‚æ•°
        if args.auto_adjust:
            batch_size, epochs, learning_rate = auto_adjust_parameters(device, args)
        else:
            batch_size = args.batch_size or 2
            epochs = args.epochs or 500
            learning_rate = args.learning_rate or 1e-4
        
        print(f"è®­ç»ƒå‚æ•°:")
        print(f"  æ•°æ®ç›®å½•: {args.data_dir}")
        print(f"  æ¨¡å‹åˆ—è¡¨: {args.model_names}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  è®­ç»ƒè½®æ•°: {epochs}")
        print(f"  å­¦ä¹ ç‡: {learning_rate}")
        print(f"  è®¡ç®—è®¾å¤‡: {device}")
        print(f"  å¯ç”¨é›†æˆ: {args.enable_ensemble}")
        print(f"  è‡ªåŠ¨è°ƒèŠ‚: {args.auto_adjust}")
        print("-" * 60)
        
        trained_models = []
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for i, model_name in enumerate(args.model_names):
            print(f"\nè®­ç»ƒæ¨¡å‹ {i+1}/{len(args.model_names)}: {model_name.upper()}")
            
            # åˆ›å»ºè®­ç»ƒé…ç½®
            config = merge_args_with_config(args, device)
            
            # æ›´æ–°æ¨¡å‹ç‰¹å®šé…ç½®
            config.update({
                'batch_size': batch_size,
                'max_epochs': epochs,
                'learning_rate': learning_rate,
                'model_name': model_name,
                'output_dir': f'{args.output_dir}/models/{model_name}'
            })
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = BraTSTrainer(config)
            
            # å¼€å§‹è®­ç»ƒ
            trainer.train()
            
            # è®°å½•è®­ç»ƒå¥½çš„æ¨¡å‹
            model_path = f'{args.output_dir}/models/{model_name}/checkpoints/best_model.pth'
            if os.path.exists(model_path):
                trained_models.append({
                    'name': model_name,
                    'path': model_path,
                    'config': config
                })
                print(f"[æˆåŠŸ] {model_name.upper()} è®­ç»ƒå®Œæˆ")
            else:
                print(f"[å¤±è´¥] {model_name.upper()} è®­ç»ƒå¤±è´¥")
        
        # æ¨¡å‹é›†æˆ
        if args.enable_ensemble and len(trained_models) > 1:
            print(f"\nå¼€å§‹åˆ›å»ºå®Œæ•´æ¨¡å‹é›†æˆ...")
            
            # å¯¼å…¥é›†æˆå‡½æ•°
            from model import create_full_ensemble
            
            # åˆ›å»ºå®Œæ•´é›†æˆæ¨¡å‹
            ensemble_model = create_full_ensemble(device=str(device))
            
            # ä¿å­˜é›†æˆæ¨¡å‹é…ç½®
            ensemble_config = {
                'model_type': 'full_ensemble',
                'models': [model['name'] for model in trained_models],
                'device': str(device),
                'created_at': str(torch.utils.data.get_worker_info() or 'main')
            }
            
            ensemble_dest = f'{args.output_dir}/checkpoints/ensemble_model.pth'
            os.makedirs(os.path.dirname(ensemble_dest), exist_ok=True)
            
            # ä¿å­˜é›†æˆæ¨¡å‹ä¿¡æ¯
            torch.save({
                'config': ensemble_config,
                'trained_models': trained_models
            }, ensemble_dest)
            
            print(f"å®Œæ•´é›†æˆæ¨¡å‹å·²åˆ›å»º")
            print(f"é›†æˆæ¨¡å‹é…ç½®å·²ä¿å­˜åˆ°: {ensemble_dest}")
            print(f"åŒ…å«æ¨¡å‹: {[model['name'].upper() for model in trained_models]}")
        
        print(f"\n[æˆåŠŸ] ç®€åŒ–è®­ç»ƒæµç¨‹å®Œæˆï¼")
        print(f"è®­ç»ƒäº† {len(trained_models)} ä¸ªæ¨¡å‹")
        if args.enable_ensemble and len(trained_models) > 1:
            print(f"å·²åˆ›å»ºå®Œæ•´æ¨¡å‹é›†æˆ")
        
        return True
        
    except Exception as e:
        print(f"\n[é”™è¯¯] ç®€åŒ–è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def auto_adjust_parameters(device: torch.device, args):
    """
    æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒèŠ‚è®­ç»ƒå‚æ•°
    
    Args:
        device: è®¡ç®—è®¾å¤‡
        args: å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        tuple: (batch_size, epochs, learning_rate)
    """
    # åŸºç¡€å‚æ•°
    base_batch_size = args.batch_size or 2
    base_epochs = args.epochs or 500
    base_lr = args.learning_rate or 1e-4
    
    if device.type == 'cuda':
        # GPUè®¾å¤‡ï¼Œä½¿ç”¨ä¼˜åŒ–å‚æ•°
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        batch_size = base_batch_size * 2  # é«˜ç«¯GPUå¯ä»¥å¤„ç†æ›´å¤§æ‰¹æ¬¡
        epochs = base_epochs
        learning_rate = base_lr * 1.2  # ç¨å¾®æé«˜å­¦ä¹ ç‡åŠ é€Ÿè®­ç»ƒ
        
        print(f"GPUä¼˜åŒ–é…ç½® (æ˜¾å­˜: {gpu_memory_gb:.1f}GB)")
    else:
        # CPUè®¾å¤‡ï¼Œä½¿ç”¨åˆç†å‚æ•°
        batch_size = base_batch_size  # CPUå¯ä»¥å¤„ç†æ­£å¸¸æ‰¹æ¬¡
        epochs = base_epochs  # ä¿æŒæ­£å¸¸è®­ç»ƒè½®æ•°
        learning_rate = base_lr  # ä½¿ç”¨æ ‡å‡†å­¦ä¹ ç‡
        
        print(f"CPUä¼˜åŒ–é…ç½®")
    
    return batch_size, epochs, learning_rate


def run_evaluation(model_path: str, data_dir: str, output_dir: str = './evaluation_results', device: torch.device = None):
    """
    è¿è¡Œè¯„ä¼°æµç¨‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        data_dir: æ•°æ®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        device: è®¡ç®—è®¾å¤‡
    """
    print("\n" + "=" * 80)
    print("ğŸ” BraTSè„‘è‚¿ç˜¤åˆ†å‰²æ¨¡å‹è¯„ä¼°")
    print("=" * 80)
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        if not os.path.exists(model_path):
            print(f"[é”™è¯¯] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        # æ˜¾ç¤ºè¯„ä¼°é…ç½®ä¿¡æ¯
        print(f"\n è¯„ä¼°é…ç½®ä¿¡æ¯:")
        print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"  æ•°æ®ç›®å½•: {data_dir}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        if device is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"\n è®¡ç®—è®¾å¤‡: {device}")
        if device.type == 'cuda':
            print(f"  GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
            print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        print("\n" + "-" * 80)
        print("åˆå§‹åŒ–è¯„ä¼°å™¨...")
        
        evaluator = BraTSEvaluator(
            model_path=model_path,
            data_dir=data_dir,
            device=device,
            output_dir=output_dir
        )
        
        print("\n" + "-" * 80)
        print(" å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        # æ‰§è¡Œè¯„ä¼°
        results = evaluator.evaluate_model()
        
        # è¯¦ç»†æ‰“å°ç»“æœ
        print("\n" + "=" * 80)
        print(" è¯„ä¼°ç»“æœè¯¦ç»†æŠ¥å‘Š")
        print("=" * 80)
        
        # Diceç³»æ•°ç»Ÿè®¡
        print(f"\n Diceç³»æ•° (åˆ†å‰²å‡†ç¡®æ€§æŒ‡æ ‡):")
        print(f"  â”œâ”€ å¹³å‡å€¼: {results['mean_dice']:.4f} Â± {results['std_dice']:.4f}")
        print(f"  â”œâ”€ ä¸­ä½æ•°: {results['median_dice']:.4f}")
        print(f"  â”œâ”€ æœ€å°å€¼: {results['min_dice']:.4f}")
        print(f"  â””â”€ æœ€å¤§å€¼: {results['max_dice']:.4f}")
        
        # Hausdorffè·ç¦»ç»Ÿè®¡
        print(f"\n Hausdorffè·ç¦» (è¾¹ç•Œå‡†ç¡®æ€§æŒ‡æ ‡):")
        print(f"  â”œâ”€ å¹³å‡å€¼: {results['mean_hd']:.4f} Â± {results['std_hd']:.4f}")
        print(f"  â””â”€ ä¸­ä½æ•°: {results['median_hd']:.4f}")
        
        # è¡¨é¢è·ç¦»ç»Ÿè®¡
        print(f"\n è¡¨é¢è·ç¦» (è¡¨é¢åŒ¹é…æŒ‡æ ‡):")
        print(f"  â”œâ”€ å¹³å‡å€¼: {results['mean_surface']:.4f} Â± {results['std_surface']:.4f}")
        print(f"  â””â”€ ä¸­ä½æ•°: {results['median_surface']:.4f}")
        
        # è¯„ä¼°æ¡ˆä¾‹ç»Ÿè®¡
        print(f"\n è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  â”œâ”€ æ€»æ¡ˆä¾‹æ•°: {results['total_cases']}")
        print(f"  â”œâ”€ æˆåŠŸè¯„ä¼°: {results['total_cases']}")
        print(f"  â””â”€ å¤±è´¥æ¡ˆä¾‹: 0")
        
        # æ€§èƒ½è¯„çº§
        mean_dice = results['mean_dice']
        if mean_dice >= 0.9:
            performance_level = "[ä¼˜ç§€] (Excellent)"
        elif mean_dice >= 0.8:
            performance_level = "[è‰¯å¥½] (Good)"
        elif mean_dice >= 0.7:
            performance_level = "[ä¸­ç­‰] (Fair)"
        elif mean_dice >= 0.6:
            performance_level = "[ä¸€èˆ¬] (Poor)"
        else:
            performance_level = "[è¾ƒå·®] (Very Poor)"
        
        print(f"\næ¨¡å‹æ€§èƒ½è¯„çº§: {performance_level}")
        
        # è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        print(f"\n è¾“å‡ºæ–‡ä»¶ä¿¡æ¯:")
        print(f"  â”œâ”€ æ¡ˆä¾‹è¯¦ç»†ç»“æœ: {output_dir}/case_results.csv")
        print(f"  â”œâ”€ æ€»ä½“ç»Ÿè®¡æŠ¥å‘Š: {output_dir}/summary_results.txt")
        print(f"  â”œâ”€ ç»“æœåˆ†å¸ƒå›¾: {output_dir}/results_distribution.png")
        print(f"  â””â”€ å¯è§†åŒ–å›¾åƒ: {output_dir}/visualizations/")
        
        print("\n" + "=" * 80)
        print("[æˆåŠŸ] æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        print(f"\n[é”™è¯¯] è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# æ¨ç†åŠŸèƒ½å·²ç§»é™¤





def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(
        description='BraTSè„‘è‚¿ç˜¤åˆ†å‰²é¡¹ç›®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""é»˜è®¤é›†æˆè®­ç»ƒï¼ˆæ‰€æœ‰7ä¸ªæ¨¡å‹ï¼Œ500è½®ï¼‰"""
    )
    
    parser.add_argument(
        '--mode', 
        type=str, 
        choices=['train', 'evaluate'],
        required=True,
        help='è¿è¡Œæ¨¡å¼: trainï¼ˆè®­ç»ƒï¼‰, evaluateï¼ˆè¯„ä¼°ï¼‰'
    )
    

    
    parser.add_argument(
        '--data_dir',
        type=str,
        required=True,
        help='æ•°æ®ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚ï¼špath/to/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2'
    )
    
    parser.add_argument(
        '--model_path',
        type=str,
        default='./outputs/checkpoints/best_model.pth',
        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--output_dir',
        type=str,
        default='./outputs',
        help='è¾“å‡ºç›®å½•'
    )
    
    parser.add_argument(
        '--batch_size',
        type=int,
        default=None,
        help='æ‰¹æ¬¡å¤§å°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--epochs',
        type=int,
        default=500,
        help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤500è½®ï¼‰'
    )
    
    parser.add_argument(
        '--learning_rate',
        type=float,
        default=None,
        help='å­¦ä¹ ç‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--model_name',
        type=str,
        choices=get_all_supported_models(),
        help='è¦è®­ç»ƒçš„å•ä¸ªæ¨¡å‹åç§°'
    )
    
    parser.add_argument(
        '--model_names',
        type=str,
        nargs='+',
        choices=get_all_supported_models(),
        help='è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨ï¼Œç”¨äºå¤šæ¨¡å‹è®­ç»ƒ'
    )
    
    parser.add_argument(
        '--enable_ensemble',
        type=lambda x: x.lower() in ['true', '1', 'yes', 'on'],
        default=False,
        help='æ˜¯å¦å¯ç”¨æ¨¡å‹é›†æˆè®­ç»ƒï¼ˆé»˜è®¤å…³é—­ï¼Œä»…åœ¨è®­ç»ƒå¤šä¸ªæ¨¡å‹æ—¶å¯ç”¨ï¼‰'
    )
    

    
    parser.add_argument(
        '--auto_adjust',
        action='store_true',
        default=True,
        help='æ˜¯å¦æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒèŠ‚å‚æ•°'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        choices=['cpu', 'cuda', 'auto'],
        default='auto',
        help='è®¡ç®—è®¾å¤‡é€‰æ‹©: cpu, cuda, autoï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰'
    )
    

    
    args = parser.parse_args()
    
    # å¤„ç†æ¨¡å‹å‚æ•°é€»è¾‘
    if args.model_name and args.model_names:
        print("[é”™è¯¯] ä¸èƒ½åŒæ—¶æŒ‡å®š --model_name å’Œ --model_names å‚æ•°")
        return 1
    elif args.model_name:
        # å•ä¸ªæ¨¡å‹è®­ç»ƒ
        args.model_names = [args.model_name]
    elif args.model_names:
        # å¤šä¸ªæ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹åˆ—è¡¨
        pass
    else:
        # é»˜è®¤è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        args.model_names = get_all_supported_models()
    
    # è‡ªåŠ¨åˆ¤æ–­é›†æˆæ¨¡å¼ï¼šå•ä¸ªæ¨¡å‹ç¦ç”¨é›†æˆï¼Œå¤šä¸ªæ¨¡å‹å¯ç”¨é›†æˆ
    if len(args.model_names) == 1:
        args.enable_ensemble = False  # å•ä¸ªæ¨¡å‹æ—¶è‡ªåŠ¨ç¦ç”¨é›†æˆ
    else:
        args.enable_ensemble = True   # å¤šä¸ªæ¨¡å‹æ—¶è‡ªåŠ¨å¯ç”¨é›†æˆ
    
    # è®¾å¤‡é…ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        device_name = 'GPU' if torch.cuda.is_available() else 'CPU'
    elif args.device == 'cuda':
        if torch.cuda.is_available():
            device = torch.device('cuda')
            device_name = 'GPU'
        else:
            print("[è­¦å‘Š] æŒ‡å®šä½¿ç”¨GPUä½†CUDAä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPU")
            device = torch.device('cpu')
            device_name = 'CPU'
    else:  # cpu
        device = torch.device('cpu')
        device_name = 'CPU'
    
    # è®¾ç½®å…¨å±€è®¾å¤‡
    torch.cuda.set_device(device) if device.type == 'cuda' else None
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 80)
    print("åŸºäºMONAIæ¡†æ¶çš„åŒ»å­¦å›¾åƒåˆ†å‰²è§£å†³æ–¹æ¡ˆ")
    print("=" * 80)
    print(f"è®¡ç®—è®¾å¤‡: {device_name} ({device})")
    if device.type == 'cuda':
        print(f"GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    if args.mode in ['evaluate', 'inference']:
        print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(args.data_dir):
        print(f"\n[é”™è¯¯] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        print("è¯·æ£€æŸ¥æ•°æ®è·¯å¾„æˆ–ä¸‹è½½BraTSæ•°æ®é›†")
        return 1
    
    success = True
    
    try:
        if args.mode == 'train':
            success = run_simplified_training(args, device)
            
        elif args.mode == 'evaluate':
            success = run_evaluation(args.model_path, args.data_dir, args.output_dir + '/evaluation', device)
            
        # æ¨ç†åŠŸèƒ½å·²ç§»é™¤
    
    except KeyboardInterrupt:
        print("\n\n[è­¦å‘Š] ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 1
    except Exception as e:
        print(f"\n\n[é”™è¯¯] æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    if success:
        print("\n" + "=" * 80)
        print("[æˆåŠŸ] æ‰§è¡Œå®Œæˆï¼")
        print("=" * 80)
        return 0
    else:
        print("\n" + "=" * 80)
        print("[å¤±è´¥] æ‰§è¡Œå¤±è´¥ï¼")
        print("=" * 80)
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)