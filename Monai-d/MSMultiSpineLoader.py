#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MS_MultiSpineæ•°æ®é›†ä¼˜åŒ–è®­ç»ƒç­–ç•¥
é’ˆå¯¹ä½å‰æ™¯æ¯”ä¾‹æ•°æ®é›†çš„è‡ªé€‚åº”è®­ç»ƒæ–¹æ¡ˆ

ä¸»è¦ä¼˜åŒ–ç­–ç•¥ï¼š
1. è‡ªé€‚åº”RandCropByPosNegLabeldå‚æ•°
2. æ¸è¿›å¼è®­ç»ƒç­–ç•¥
3. æ•°æ®ä¸å¹³è¡¡ä¼˜åŒ–æŸå¤±å‡½æ•°
4. æ™ºèƒ½æ•°æ®å¢å¼ºç­–ç•¥
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Union
from pathlib import Path

from monai.transforms import (
    Compose,
    LoadImaged,
    EnsureChannelFirstd,
    Orientationd,
    Spacingd,
    ScaleIntensityRanged,
    CropForegroundd,
    RandCropByPosNegLabeld,
    RandFlipd,
    RandRotate90d,
    RandShiftIntensityd,
    RandAffined,
    ToTensord,
    Resized,
    NormalizeIntensityd,
    RandGaussianNoised,
    RandGaussianSmoothd,
    RandAdjustContrastd,
    RandZoomd,
    Rand3DElasticd,
    RandBiasFieldd,
    Lambda
)
from monai.data import Dataset, CacheDataset, DataLoader as MonaiDataLoader
from torch.utils.data import DataLoader
from monai.losses import DiceLoss, FocalLoss, DiceCELoss
from monai.metrics import DiceMetric
from monai.utils import set_determinism

class AdaptiveRandCropByPosNegLabeld:
    """
    è‡ªé€‚åº”çš„æ­£è´Ÿæ ·æœ¬è£å‰ªç­–ç•¥
    æ ¹æ®æ•°æ®é›†çš„å‰æ™¯æ¯”ä¾‹åŠ¨æ€è°ƒæ•´å‚æ•°
    """
    
    def __init__(self, 
                 keys: List[str],
                 label_key: str,
                 spatial_size: Tuple[int, int, int],
                 image_key: str = "image",
                 foreground_ratio_threshold: float = 0.001,
                 adaptive_mode: str = "progressive"):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”è£å‰ªç­–ç•¥
        
        Args:
            keys: è¦å¤„ç†çš„é”®åˆ—è¡¨
            label_key: æ ‡ç­¾é”®å
            spatial_size: è£å‰ªå°ºå¯¸
            image_key: å›¾åƒé”®å
            foreground_ratio_threshold: å‰æ™¯æ¯”ä¾‹é˜ˆå€¼
            adaptive_mode: è‡ªé€‚åº”æ¨¡å¼ ('progressive', 'flexible', 'background_aware')
        """
        self.keys = keys
        self.label_key = label_key
        self.spatial_size = spatial_size
        self.image_key = image_key
        self.foreground_ratio_threshold = foreground_ratio_threshold
        self.adaptive_mode = adaptive_mode
        
    def __call__(self, data: Dict) -> Dict:
        """
        æ‰§è¡Œè‡ªé€‚åº”è£å‰ª
        """
        # è®¡ç®—å‰æ™¯æ¯”ä¾‹
        label = data[self.label_key]
        if isinstance(label, torch.Tensor):
            label_np = label.cpu().numpy()
        else:
            label_np = np.array(label)
            
        total_voxels = np.prod(label_np.shape)
        foreground_voxels = np.sum(label_np > 0)
        foreground_ratio = foreground_voxels / total_voxels if total_voxels > 0 else 0
        
        # æ ¹æ®å‰æ™¯æ¯”ä¾‹é€‰æ‹©ç­–ç•¥
        if foreground_ratio < self.foreground_ratio_threshold:
            # æä½å‰æ™¯æ¯”ä¾‹ï¼šä½¿ç”¨èƒŒæ™¯æ„ŸçŸ¥ç­–ç•¥
            return self._background_aware_crop(data, foreground_ratio)
        elif foreground_ratio < 0.01:
            # ä½å‰æ™¯æ¯”ä¾‹ï¼šä½¿ç”¨çµæ´»ç­–ç•¥
            return self._flexible_crop(data, foreground_ratio)
        else:
            # æ­£å¸¸å‰æ™¯æ¯”ä¾‹ï¼šä½¿ç”¨æ ‡å‡†ç­–ç•¥
            return self._standard_crop(data)
    
    def _background_aware_crop(self, data: Dict, foreground_ratio: float) -> Dict:
        """
        èƒŒæ™¯æ„ŸçŸ¥è£å‰ªï¼šä¸»è¦ç”ŸæˆèƒŒæ™¯æ ·æœ¬ï¼Œå¶å°”å°è¯•å‰æ™¯
        """
        # å¯¹äºæ— å‰æ™¯æˆ–æå°‘å‰æ™¯çš„æ ·æœ¬ï¼Œä¸»è¦ä½¿ç”¨èƒŒæ™¯è£å‰ª
        if foreground_ratio == 0:
            # å®Œå…¨æ— å‰æ™¯ï¼šåªç”ŸæˆèƒŒæ™¯æ ·æœ¬
            crop_transform = RandCropByPosNegLabeld(
                keys=self.keys,
                label_key=self.label_key,
                spatial_size=self.spatial_size,
                pos=0,  # ä¸è¦æ±‚å‰æ™¯
                neg=2,  # ç”Ÿæˆ2ä¸ªèƒŒæ™¯æ ·æœ¬
                num_samples=2,
                image_key=self.image_key,
                image_threshold=0,
                allow_smaller=True  # å…è®¸æ›´å°çš„è£å‰ª
            )
        else:
            # æå°‘å‰æ™¯ï¼šå¶å°”å°è¯•å‰æ™¯ï¼Œä¸»è¦ä½¿ç”¨èƒŒæ™¯
            crop_transform = RandCropByPosNegLabeld(
                keys=self.keys,
                label_key=self.label_key,
                spatial_size=self.spatial_size,
                pos=0.2,  # 20%æ¦‚ç‡å°è¯•å‰æ™¯
                neg=1.8,  # 80%ä½¿ç”¨èƒŒæ™¯
                num_samples=2,
                image_key=self.image_key,
                image_threshold=0,
                allow_smaller=True
            )
        
        return crop_transform(data)
    
    def _flexible_crop(self, data: Dict, foreground_ratio: float) -> Dict:
        """
        çµæ´»è£å‰ªï¼šæ ¹æ®å‰æ™¯æ¯”ä¾‹åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        """
        # åŠ¨æ€è®¡ç®—poså’Œnegæ¯”ä¾‹
        pos_ratio = min(0.5, foreground_ratio * 50)  # å‰æ™¯æ¯”ä¾‹çš„50å€ï¼Œæœ€å¤§0.5
        neg_ratio = 2 - pos_ratio
        
        crop_transform = RandCropByPosNegLabeld(
            keys=self.keys,
            label_key=self.label_key,
            spatial_size=self.spatial_size,
            pos=pos_ratio,
            neg=neg_ratio,
            num_samples=2,
            image_key=self.image_key,
            image_threshold=0,
            allow_smaller=True
        )
        
        return crop_transform(data)
    
    def _standard_crop(self, data: Dict) -> Dict:
        """
        æ ‡å‡†è£å‰ªï¼šä½¿ç”¨åŸå§‹çš„å¹³è¡¡ç­–ç•¥
        """
        crop_transform = RandCropByPosNegLabeld(
            keys=self.keys,
            label_key=self.label_key,
            spatial_size=self.spatial_size,
            pos=1,
            neg=1,
            num_samples=2,
            image_key=self.image_key,
            image_threshold=0
        )
        
        return crop_transform(data)

class ProgressiveDataAugmentation:
    """
    æ¸è¿›å¼æ•°æ®å¢å¼ºç­–ç•¥
    è®­ç»ƒåˆæœŸä½¿ç”¨è½»åº¦å¢å¼ºï¼ŒåæœŸé€æ¸å¢å¼º
    """
    
    def __init__(self, total_epochs: int):
        self.total_epochs = total_epochs
        self.current_epoch = 0
        
    def set_epoch(self, epoch: int):
        """è®¾ç½®å½“å‰è®­ç»ƒè½®æ¬¡"""
        self.current_epoch = epoch
        
    def get_augmentation_intensity(self) -> float:
        """è·å–å½“å‰å¢å¼ºå¼ºåº¦ (0.0-1.0)"""
        if self.total_epochs <= 1:
            return 0.5
        
        # å‰30%è½®æ¬¡ä½¿ç”¨è½»åº¦å¢å¼ºï¼Œå70%é€æ¸å¢å¼º
        if self.current_epoch < self.total_epochs * 0.3:
            return 0.3  # è½»åº¦å¢å¼º
        else:
            # çº¿æ€§å¢é•¿åˆ°æœ€å¤§å¼ºåº¦
            progress = (self.current_epoch - self.total_epochs * 0.3) / (self.total_epochs * 0.7)
            return 0.3 + 0.4 * progress  # ä»0.3å¢é•¿åˆ°0.7
    
    def get_progressive_transforms(self, base_transforms: List, mode: str = "train") -> Compose:
        """è·å–æ¸è¿›å¼å˜æ¢"""
        if mode != "train":
            return Compose(base_transforms + [ToTensord(keys=["image", "label"])])
        
        intensity = self.get_augmentation_intensity()
        
        # è‡ªé€‚åº”è£å‰ªï¼ˆå§‹ç»ˆä½¿ç”¨ï¼‰
        adaptive_crop = AdaptiveRandCropByPosNegLabeld(
            keys=["image", "label"],
            label_key="label",
            spatial_size=(128, 128, 128),
            image_key="image"
        )
        
        # æ¸è¿›å¼å¢å¼ºå˜æ¢
        progressive_transforms = [
            # å‡ ä½•å˜æ¢ï¼ˆå¼ºåº¦éšè®­ç»ƒè¿›åº¦å¢åŠ ï¼‰
            RandFlipd(
                keys=["image", "label"],
                spatial_axis=[0, 1, 2],
                prob=0.05 + 0.15 * intensity  # ä»5%å¢é•¿åˆ°20%
            ),
            RandRotate90d(
                keys=["image", "label"],
                prob=0.05 + 0.15 * intensity,
                max_k=3
            ),
            RandAffined(
                keys=['image', 'label'],
                mode=('bilinear', 'nearest'),
                prob=0.1 + 0.2 * intensity,  # ä»10%å¢é•¿åˆ°30%
                spatial_size=(128, 128, 128),
                rotate_range=(0, 0, np.pi / 30 * intensity),  # æ—‹è½¬è§’åº¦é€æ¸å¢åŠ 
                scale_range=(0.05 * intensity, 0.05 * intensity, 0.05 * intensity)
            ),
            # å¼ºåº¦å˜æ¢ï¼ˆé€‚åº¦ä½¿ç”¨ï¼‰
            RandShiftIntensityd(
                keys=["image"],
                offsets=0.05 + 0.1 * intensity,  # ä»5%å¢é•¿åˆ°15%
                prob=0.3 + 0.2 * intensity
            ),
            RandGaussianNoised(
                keys=["image"],
                prob=0.1 * intensity,  # å™ªå£°æ¦‚ç‡è¾ƒä½
                mean=0.0,
                std=0.05 * intensity
            ),
            RandAdjustContrastd(
                keys=["image"],
                prob=0.1 + 0.1 * intensity,
                gamma=(0.8 + 0.1 * intensity, 1.2 - 0.1 * intensity)
            )
        ]
        
        # ç»„åˆæ‰€æœ‰å˜æ¢
        all_transforms = base_transforms + [adaptive_crop] + progressive_transforms + [ToTensord(keys=["image", "label"])]
        
        return Compose(all_transforms)

class ImbalancedLoss(nn.Module):
    """
    é’ˆå¯¹æ•°æ®ä¸å¹³è¡¡çš„å¤åˆæŸå¤±å‡½æ•°
    ç»“åˆDice Lossã€Focal Losså’Œæƒé‡è°ƒæ•´
    """
    
    def __init__(self, 
                 num_classes: int = 6,
                 dice_weight: float = 0.5,
                 focal_weight: float = 0.3,
                 ce_weight: float = 0.2,
                 focal_gamma: float = 2.0,
                 class_weights: Optional[List[float]] = None):
        super().__init__()
        
        self.num_classes = num_classes
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight
        self.ce_weight = ce_weight
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.dice_loss = DiceLoss(
            include_background=True,
            to_onehot_y=True,
            softmax=True,
            squared_pred=True
        )
        
        self.focal_loss = FocalLoss(
            include_background=True,
            to_onehot_y=True,
            gamma=focal_gamma,
            weight=torch.tensor(class_weights) if class_weights else None
        )
        
        self.ce_loss = nn.CrossEntropyLoss(
            weight=torch.tensor(class_weights) if class_weights else None
        )
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å¤åˆæŸå¤±
        
        Args:
            pred: é¢„æµ‹ç»“æœ [B, C, H, W, D]
            target: çœŸå®æ ‡ç­¾ [B, 1, H, W, D] æˆ– [B, H, W, D]
            
        Returns:
            å¤åˆæŸå¤±å€¼
        """
        # ç¡®ä¿targetç»´åº¦æ­£ç¡®
        if target.dim() == 4:  # [B, H, W, D]
            target = target.unsqueeze(1)  # [B, 1, H, W, D]
        
        # è®¡ç®—å„é¡¹æŸå¤±
        dice_loss_val = self.dice_loss(pred, target)
        focal_loss_val = self.focal_loss(pred, target)
        
        # CEæŸå¤±éœ€è¦ç‰¹æ®Šå¤„ç†
        target_ce = target.squeeze(1).long()  # [B, H, W, D]
        ce_loss_val = self.ce_loss(pred, target_ce)
        
        # åŠ æƒç»„åˆ
        total_loss = (
            self.dice_weight * dice_loss_val +
            self.focal_weight * focal_loss_val +
            self.ce_weight * ce_loss_val
        )
        
        return total_loss

class MSMultiSpineDatasetLoader:
    """
    MS_MultiSpineæ•°æ®é›†åŠ è½½å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    é›†æˆæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”è£å‰ªã€æ¸è¿›å¼å¢å¼ºã€å¤åˆæŸå¤±å‡½æ•°ç­‰
    
    ç‰¹ç‚¹ï¼š
    - è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å‘½åæ¨¡å¼å’Œå‰ç¼€
    - æ”¯æŒä»»æ„ç¼–å·èŒƒå›´å’Œæ–‡ä»¶å‰ç¼€ç»„åˆ
    - å…¼å®¹å¤šç§MRIåºåˆ—ç±»å‹ï¼ˆT2 + STIR/PSIR/MP2RAGEç­‰ï¼‰
    - çµæ´»é€‚åº”æ•°æ®é›†æ‰©å±•
    - é’ˆå¯¹ä½å‰æ™¯æ¯”ä¾‹æ•°æ®é›†ä¼˜åŒ–
    """
    
    def __init__(self, 
                 data_dir: str,
                 cache_rate: float = 0.0,  # é™ä½ç¼“å­˜ä»¥èŠ‚çœå†…å­˜
                 num_workers: int = 0,
                 seed: int = 42,
                 total_epochs: int = 10):
        
        self.data_dir = data_dir
        self.cache_rate = cache_rate
        self.num_workers = num_workers
        self.total_epochs = total_epochs
        
        # è®¾ç½®éšæœºç§å­
        set_determinism(seed=seed)
        
        # åˆå§‹åŒ–æ¸è¿›å¼å¢å¼º
        self.progressive_aug = ProgressiveDataAugmentation(total_epochs)
        
        # æ•°æ®è´¨é‡ç»Ÿè®¡
        self.data_quality_stats = None
        
    def analyze_data_quality(self, data_files: List[Dict]) -> Dict:
        """
        åˆ†ææ•°æ®è´¨é‡ï¼Œä¸ºä¼˜åŒ–ç­–ç•¥æä¾›ä¾æ®
        """
        print("æ­£åœ¨åˆ†ææ•°æ®è´¨é‡...")
        
        foreground_ratios = []
        quality_scores = []
        
        for data_dict in data_files[:10]:  # é‡‡æ ·åˆ†æå‰10ä¸ªæ–‡ä»¶
            try:
                # ç®€å•åŠ è½½æ ‡ç­¾è¿›è¡Œåˆ†æ
                import nibabel as nib
                label_path = data_dict['label']
                label_img = nib.load(label_path)
                label_data = label_img.get_fdata()
                
                total_voxels = np.prod(label_data.shape)
                foreground_voxels = np.sum(label_data > 0)
                foreground_ratio = foreground_voxels / total_voxels if total_voxels > 0 else 0
                
                foreground_ratios.append(foreground_ratio)
                
                # ç®€å•è´¨é‡è¯„åˆ†
                quality_score = 100 if foreground_ratio > 0.001 else 50 if foreground_ratio > 0 else 0
                quality_scores.append(quality_score)
                
            except Exception as e:
                print(f"åˆ†ææ–‡ä»¶ {data_dict.get('subject_id', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        stats = {
            'avg_foreground_ratio': np.mean(foreground_ratios) if foreground_ratios else 0,
            'min_foreground_ratio': np.min(foreground_ratios) if foreground_ratios else 0,
            'max_foreground_ratio': np.max(foreground_ratios) if foreground_ratios else 0,
            'avg_quality_score': np.mean(quality_scores) if quality_scores else 0,
            'low_quality_count': sum(1 for score in quality_scores if score < 60),
            'total_analyzed': len(foreground_ratios)
        }
        
        self.data_quality_stats = stats
        
        print(f"æ•°æ®è´¨é‡åˆ†æå®Œæˆ:")
        print(f"  å¹³å‡å‰æ™¯æ¯”ä¾‹: {stats['avg_foreground_ratio']*100:.3f}%")
        print(f"  ä½è´¨é‡æ ·æœ¬æ•°: {stats['low_quality_count']}/{stats['total_analyzed']}")
        
        return stats
    
    def get_optimized_transforms(self, mode: str = "train", epoch: int = 0) -> Compose:
        """
        è·å–ä¼˜åŒ–çš„æ•°æ®å˜æ¢
        """
        # æ›´æ–°æ¸è¿›å¼å¢å¼ºçš„è½®æ¬¡
        self.progressive_aug.set_epoch(epoch)
        
        # åŸºç¡€å˜æ¢
        base_transforms = [
            LoadImaged(keys=["image", "label"]),
            EnsureChannelFirstd(keys=["image", "label"]),
            Orientationd(keys=["image", "label"], axcodes="RAS"),
            Spacingd(
                keys=["image", "label"],
                pixdim=(1.0, 1.0, 1.0),
                mode=("bilinear", "nearest")
            ),
            ScaleIntensityRanged(
                keys=["image"],
                a_min=-200,
                a_max=400,
                b_min=0.0,
                b_max=1.0,
                clip=True
            ),
            CropForegroundd(keys=["image", "label"], source_key="image"),
            Resized(
                keys=["image", "label"],
                spatial_size=(128, 128, 128),
                mode=("trilinear", "nearest")
            ),
            NormalizeIntensityd(keys=["image"], nonzero=True, channel_wise=True),
            # æ ‡ç­¾é‡æ˜ å°„
            Lambda(self._remap_labels)
        ]
        
        # è·å–æ¸è¿›å¼å˜æ¢
        return self.progressive_aug.get_progressive_transforms(base_transforms, mode)
    
    def _remap_labels(self, data):
        """æ ‡ç­¾é‡æ˜ å°„ï¼šå°†æ ‡ç­¾6æ˜ å°„åˆ°æ ‡ç­¾5"""
        label = data["label"]
        if isinstance(label, torch.Tensor):
            label = torch.where(label == 6, 5, label)
            label = torch.clamp(label, 0, 5)
        else:
            label = np.where(label == 6, 5, label)
            label = np.clip(label, 0, 5)
        data["label"] = label
        return data
    
    def get_optimized_loss_function(self) -> ImbalancedLoss:
        """
        è·å–ä¼˜åŒ–çš„æŸå¤±å‡½æ•°
        """
        # æ ¹æ®æ•°æ®è´¨é‡è°ƒæ•´ç±»åˆ«æƒé‡
        if self.data_quality_stats:
            avg_fg_ratio = self.data_quality_stats['avg_foreground_ratio']
            # ä¸ºå‰æ™¯ç±»åˆ«åˆ†é…æ›´é«˜æƒé‡
            bg_weight = 1.0
            fg_weight = min(10.0, 1.0 / max(avg_fg_ratio, 0.001))  # å‰æ™¯æƒé‡åæ¯”äºå‰æ™¯æ¯”ä¾‹
            
            # MS_MultiSpineæœ‰6ä¸ªç±»åˆ« (0-5)
            class_weights = [bg_weight, fg_weight, fg_weight, fg_weight, fg_weight, fg_weight]
        else:
            # é»˜è®¤æƒé‡
            class_weights = [1.0, 5.0, 5.0, 5.0, 5.0, 5.0]
        
        return ImbalancedLoss(
            num_classes=6,
            dice_weight=0.5,
            focal_weight=0.3,
            ce_weight=0.2,
            focal_gamma=2.0,
            class_weights=class_weights
        )
    
    def get_training_recommendations(self) -> Dict:
        """
        åŸºäºæ•°æ®åˆ†ææä¾›è®­ç»ƒå»ºè®®
        """
        recommendations = {
            'batch_size': 1,  # å°æ‰¹æ¬¡ä»¥é€‚åº”å†…å­˜é™åˆ¶
            'learning_rate': 1e-4,  # è¾ƒå°çš„å­¦ä¹ ç‡
            'optimizer': 'AdamW',
            'scheduler': 'CosineAnnealingLR',
            'early_stopping_patience': 10,
            'gradient_clipping': 1.0,
            'mixed_precision': True,  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            'cache_rate': 0.0,  # ä¸ä½¿ç”¨ç¼“å­˜ä»¥èŠ‚çœå†…å­˜
        }
        
        if self.data_quality_stats:
            avg_fg_ratio = self.data_quality_stats['avg_foreground_ratio']
            low_quality_ratio = self.data_quality_stats['low_quality_count'] / max(self.data_quality_stats['total_analyzed'], 1)
            
            # æ ¹æ®æ•°æ®è´¨é‡è°ƒæ•´å»ºè®®
            if avg_fg_ratio < 0.001:
                recommendations.update({
                    'learning_rate': 5e-5,  # æ›´å°çš„å­¦ä¹ ç‡
                    'early_stopping_patience': 15,  # æ›´å¤§çš„è€å¿ƒ
                    'warmup_epochs': 3,  # æ·»åŠ é¢„çƒ­
                })
            
            if low_quality_ratio > 0.5:
                recommendations.update({
                    'data_filtering': True,  # å»ºè®®è¿‡æ»¤ä½è´¨é‡æ•°æ®
                    'augmentation_intensity': 'high',  # å¢å¼ºæ•°æ®å¢å¼º
                })
        
        return recommendations
    
    def _detect_file_pattern(self, case_path: str, case_id: str) -> Optional[Tuple[str, str]]:
        """
        è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ¨¡å¼ï¼Œè¿”å›(prefix, second_modality)
        é€šè¿‡æ‰«æå®é™…æ–‡ä»¶æ¥ç¡®å®šå‰ç¼€å’Œæ¨¡æ€ç±»å‹
        """
        try:
            import glob
            # è·å–ç›®å½•ä¸­æ‰€æœ‰.nii.gzæ–‡ä»¶
            nii_files = glob.glob(os.path.join(case_path, "*.nii.gz"))
            if not nii_files:
                return None
            
            # æå–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
            filenames = [os.path.basename(f).replace('.nii.gz', '') for f in nii_files]
            
            # å¯»æ‰¾T2æ–‡ä»¶æ¥ç¡®å®šå‰ç¼€
            t2_files = [f for f in filenames if f.endswith('_T2')]
            if not t2_files:
                return None
            
            # ä»T2æ–‡ä»¶åæå–å‰ç¼€ (ä¾‹å¦‚: "11-001_T2" -> prefix="11")
            t2_filename = t2_files[0]
            parts = t2_filename.split('-')
            if len(parts) < 2:
                return None
            prefix = parts[0]
            
            # å¯»æ‰¾ç¬¬äºŒæ¨¡æ€æ–‡ä»¶
            # æ’é™¤T2å’ŒLESIONMASKï¼Œå‰©ä¸‹çš„å°±æ˜¯ç¬¬äºŒæ¨¡æ€
            second_modality_files = []
            for filename in filenames:
                if (filename.startswith(f"{prefix}-{case_id}_") and 
                    not filename.endswith('_T2') and 
                    not filename.endswith('_LESIONMASK')):
                    # æå–æ¨¡æ€åç§° (ä¾‹å¦‚: "11-001_STIR" -> "STIR")
                    modality = filename.split('_')[-1]
                    second_modality_files.append(modality)
            
            if not second_modality_files:
                return None
            
            # è¿”å›æ£€æµ‹åˆ°çš„å‰ç¼€å’Œç¬¬äºŒæ¨¡æ€
            second_modality = second_modality_files[0]
            return prefix, second_modality
            
        except Exception as e:
            print(f"æ£€æµ‹æ–‡ä»¶æ¨¡å¼æ—¶å‡ºé”™ {case_path}: {e}")
            return None
    
    def get_data_dicts(self) -> Tuple[List[Dict], List[Dict]]:
        """
        è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®å­—å…¸
        ä½¿ç”¨è‡ªåŠ¨æ–‡ä»¶æ£€æµ‹ï¼Œæ”¯æŒä»»æ„ç¼–å·èŒƒå›´å’Œæ–‡ä»¶å‰ç¼€ç»„åˆ
        train_files: è®­ç»ƒæ•°æ®åˆ—è¡¨
        val_files: éªŒè¯æ•°æ®åˆ—è¡¨
        """
        data_files = []
        
        # æ‰«ææ•°æ®ç›®å½•
        if os.path.exists(self.data_dir):
            for case_dir in sorted(os.listdir(self.data_dir)):
                case_path = os.path.join(self.data_dir, case_dir)
                
                # è·³è¿‡éç›®å½•æ–‡ä»¶
                if not os.path.isdir(case_path):
                    continue
                
                # æå–ç—…ä¾‹ç¼–å· (sub-001 -> 001)
                if case_dir.startswith('sub-'):
                    case_id = case_dir[4:]  # å»æ‰'sub-'å‰ç¼€
                else:
                    print(f"è­¦å‘Š: ç›®å½•åæ ¼å¼ä¸æ­£ç¡® {case_dir}ï¼Œè·³è¿‡ï¼ˆåº”ä¸ºsub-xxxæ ¼å¼ï¼‰")
                    continue
                
                # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ¨¡å¼
                pattern_result = self._detect_file_pattern(case_path, case_id)
                if pattern_result is None:
                    print(f"è­¦å‘Š: æ— æ³•æ£€æµ‹ç—…ä¾‹ {case_dir} çš„æ–‡ä»¶æ¨¡å¼ï¼Œè·³è¿‡")
                    continue
                
                prefix, second_modality = pattern_result
                
                # æ„å»ºMS_MultiSpineæ–‡ä»¶è·¯å¾„
                t2_path = os.path.join(case_path, f"{prefix}-{case_id}_T2.nii.gz")
                second_modality_path = os.path.join(case_path, f"{prefix}-{case_id}_{second_modality}.nii.gz")
                mask_path = os.path.join(case_path, f"{prefix}-{case_id}_LESIONMASK.nii.gz")
                
                # æ£€æŸ¥æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                required_files = [t2_path, second_modality_path, mask_path]
                if all(os.path.exists(f) for f in required_files):
                    data_dict = {
                        'image': [t2_path, second_modality_path],  # 2ä¸ªæ¨¡æ€ï¼šT2å’Œç¬¬äºŒæ¨¡æ€
                        'label': mask_path,
                        'subject_id': case_dir,
                        'modalities': ['T2', second_modality],  # è®°å½•å®é™…ä½¿ç”¨çš„æ¨¡æ€
                        'prefix': prefix  # è®°å½•æ£€æµ‹åˆ°çš„å‰ç¼€
                    }
                    data_files.append(data_dict)
                    print(f"æˆåŠŸåŠ è½½ç—…ä¾‹ {case_dir}: T2 + {second_modality} (å‰ç¼€: {prefix})")
                else:
                    missing_files = [f for f in required_files if not os.path.exists(f)]
                    print(f"è­¦å‘Š: ç—…ä¾‹ {case_dir} ç¼ºå°‘æ–‡ä»¶: {[os.path.basename(f) for f in missing_files]}ï¼Œè·³è¿‡")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œç›´æ¥è­¦å‘Šå¹¶è¿”å›ç©ºåˆ—è¡¨
        if not data_files:
            print("è­¦å‘Š: æœªæ‰¾åˆ°MS_MultiSpineæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print(f"æ•°æ®è·¯å¾„: {self.data_dir}")
            print("è¯·ç¡®ä¿æ•°æ®ç›®å½•åŒ…å«sub-*æ ¼å¼çš„ç—…ä¾‹æ–‡ä»¶å¤¹")
            return [], []
        
        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
        split_idx = int(0.8 * len(data_files))
        train_files = data_files[:split_idx]
        val_files = data_files[split_idx:]
        
        print(f"æ‰¾åˆ° {len(data_files)} ä¸ªå®Œæ•´çš„MS_MultiSpineç—…ä¾‹")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_files)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_files)}")
        
        return train_files, val_files
    
    def get_transforms(self, mode: str = "train", epoch: int = 0) -> Compose:
        """
        è·å–æ•°æ®å˜æ¢æµç¨‹ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
        é€‚é…è„ŠæŸ±MRIçš„ç‰¹ç‚¹ï¼Œé›†æˆä¼˜åŒ–ç­–ç•¥
        """
        return self.get_optimized_transforms(mode, epoch)
    
    def get_dataloaders(self, batch_size: int = 2) -> Tuple[DataLoader, DataLoader]:
        """
        è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        batch_size: æ‰¹æ¬¡å¤§å°
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        """
        # è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨
        train_files, val_files = self.get_data_dicts()
        return self.create_dataloaders_from_dicts(train_files, val_files, batch_size)
    
    def create_dataloaders_from_dicts(self, train_files: List[Dict], val_files: List[Dict], 
                                     batch_size: int = 2) -> Tuple[DataLoader, DataLoader]:
        """
        ä»ç»™å®šçš„æ•°æ®å­—å…¸åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_files: è®­ç»ƒæ•°æ®å­—å…¸åˆ—è¡¨
        val_files: éªŒè¯æ•°æ®å­—å…¸åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        """
        # è·å–å˜æ¢
        train_transforms = self.get_transforms("train")
        val_transforms = self.get_transforms("val")
        
        # åˆ›å»ºæ•°æ®é›†
        if self.cache_rate > 0:
            # ç¼“å­˜æ•°æ®é›†
            train_ds = CacheDataset(
                data=train_files,
                transform=train_transforms,
                cache_rate=self.cache_rate,
                num_workers=self.num_workers
            )
            val_ds = CacheDataset(
                data=val_files,
                transform=val_transforms,
                cache_rate=self.cache_rate,
                num_workers=self.num_workers
            )
        else:
            # ä½¿ç”¨æ™®é€šæ•°æ®é›†
            train_ds = Dataset(data=train_files, transform=train_transforms)
            val_ds = Dataset(data=val_files, transform=val_transforms)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - æ·»åŠ Windowså…¼å®¹æ€§è®¾ç½®
        train_loader = MonaiDataLoader(
            train_ds,
            batch_size=batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=False,  # Windowså…¼å®¹æ€§è®¾ç½®
            multiprocessing_context=None  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        val_loader = MonaiDataLoader(
            val_ds,
            batch_size=batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=False,  # Windowså…¼å®¹æ€§è®¾ç½®
            multiprocessing_context=None  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        return train_loader, val_loader
    
    def print_data_info(self, dataloader: DataLoader):
        """
        æ‰“å°æ•°æ®åŠ è½½å™¨ä¿¡æ¯
        """
        print(f"æ•°æ®é›†å¤§å°: {len(dataloader.dataset)}")
        print(f"æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
        print(f"æ‰¹æ¬¡å¤§å°: {dataloader.batch_size}")
        
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æŸ¥çœ‹å½¢çŠ¶
        for batch in dataloader:
            print(f"å›¾åƒå½¢çŠ¶: {batch['image'].shape}")
            print(f"æ ‡ç­¾å½¢çŠ¶: {batch['label'].shape}")
            print(f"å›¾åƒæ•°æ®ç±»å‹: {batch['image'].dtype}")
            print(f"æ ‡ç­¾æ•°æ®ç±»å‹: {batch['label'].dtype}")
            print(f"å›¾åƒå€¼èŒƒå›´: [{batch['image'].min():.3f}, {batch['image'].max():.3f}]")
            print(f"æ ‡ç­¾å”¯ä¸€å€¼: {torch.unique(batch['label'])}")
            break

def create_optimized_training_config(data_dir: str, 
                                   total_epochs: int = 10,
                                   device: str = "cpu") -> Dict:
    """
    åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        total_epochs: æ€»è®­ç»ƒè½®æ¬¡
        device: è®­ç»ƒè®¾å¤‡
        
    Returns:
        ä¼˜åŒ–çš„è®­ç»ƒé…ç½®å­—å…¸
    """
    print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®...")
    
    # åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    loader = MSMultiSpineDatasetLoader(
        data_dir=data_dir,
        cache_rate=0.0,
        num_workers=0,
        total_epochs=total_epochs
    )
    
    # æ¨¡æ‹Ÿæ•°æ®æ–‡ä»¶åˆ—è¡¨ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®æ•°æ®ï¼‰
    data_files = [{'label': os.path.join(data_dir, f'sub-{i:03d}', f'*_LESIONMASK.nii.gz')} 
                  for i in range(1, 11)]  # å‰10ä¸ªæ ·æœ¬
    
    # åˆ†ææ•°æ®è´¨é‡
    try:
        loader.analyze_data_quality(data_files)
    except Exception as e:
        print(f"æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # è·å–è®­ç»ƒå»ºè®®
    recommendations = loader.get_training_recommendations()
    
    # åˆ›å»ºé…ç½®
    config = {
        'data_loader': loader,
        'loss_function': loader.get_optimized_loss_function(),
        'training_params': recommendations,
        'transforms': {
            'train': lambda epoch: loader.get_optimized_transforms('train', epoch),
            'val': lambda epoch: loader.get_optimized_transforms('val', epoch)
        },
        'device': device,
        'total_epochs': total_epochs
    }
    
    print("ä¼˜åŒ–é…ç½®åˆ›å»ºå®Œæˆï¼")
    print("\n=== è®­ç»ƒå»ºè®® ===")
    for key, value in recommendations.items():
        print(f"{key}: {value}")
    
    return config

def print_optimization_summary():
    """
    æ‰“å°ä¼˜åŒ–ç­–ç•¥æ€»ç»“
    """
    print("\n" + "="*80)
    print("MS_MultiSpineæ•°æ®é›†ä¼˜åŒ–è®­ç»ƒç­–ç•¥æ€»ç»“")
    print("="*80)
    
    print("\nğŸ¯ ä¸»è¦ä¼˜åŒ–ç­–ç•¥:")
    print("1. è‡ªé€‚åº”RandCropByPosNegLabeld:")
    print("   - æ— å‰æ™¯æ ·æœ¬: åªç”ŸæˆèƒŒæ™¯æ ·æœ¬ (pos=0, neg=2)")
    print("   - æå°‘å‰æ™¯: åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ¯”ä¾‹ (pos=0.2, neg=1.8)")
    print("   - æ­£å¸¸å‰æ™¯: ä½¿ç”¨æ ‡å‡†å¹³è¡¡ç­–ç•¥ (pos=1, neg=1)")
    
    print("\n2. æ¸è¿›å¼æ•°æ®å¢å¼º:")
    print("   - å‰30%è½®æ¬¡: è½»åº¦å¢å¼º (å¼ºåº¦0.3)")
    print("   - å70%è½®æ¬¡: é€æ¸å¢å¼º (å¼ºåº¦0.3â†’0.7)")
    print("   - å‡ ä½•å˜æ¢æ¦‚ç‡: 5%â†’20%")
    print("   - å¼ºåº¦å˜æ¢æ¦‚ç‡: 5%â†’15%")
    
    print("\n3. æ•°æ®ä¸å¹³è¡¡æŸå¤±å‡½æ•°:")
    print("   - Dice Loss (50%) + Focal Loss (30%) + CE Loss (20%)")
    print("   - å‰æ™¯ç±»åˆ«æƒé‡: 5å€äºèƒŒæ™¯")
    print("   - Focal Loss gamma=2.0 å¤„ç†éš¾æ ·æœ¬")
    
    print("\n4. è®­ç»ƒå‚æ•°ä¼˜åŒ–:")
    print("   - æ‰¹æ¬¡å¤§å°: 1 (é€‚åº”å†…å­˜é™åˆ¶)")
    print("   - å­¦ä¹ ç‡: 1e-4 (ä½å‰æ™¯æ¯”ä¾‹æ—¶é™è‡³5e-5)")
    print("   - ç¼“å­˜ç‡: 0.0 (èŠ‚çœå†…å­˜)")
    print("   - æ··åˆç²¾åº¦è®­ç»ƒ: å¯ç”¨")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. ä¸è¿‡æ»¤æ•°æ®ï¼Œæœ€å¤§åŒ–åˆ©ç”¨æ‰€æœ‰æ ·æœ¬")
    print("2. ä½¿ç”¨CPUè®­ç»ƒä»¥é¿å…å†…å­˜ä¸è¶³")
    print("3. ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼Œé€‚æ—¶è°ƒæ•´å‚æ•°")
    print("4. ä½¿ç”¨æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ")
    
    print("\nğŸ“Š é¢„æœŸæ•ˆæœ:")
    print("- å‡å°‘'Num foregrounds 0'è­¦å‘Š")
    print("- æé«˜è®­ç»ƒç¨³å®šæ€§")
    print("- æ›´å¥½åœ°åˆ©ç”¨å°æ•°æ®é›†")
    print("- é€‚åº”æ•°æ®ä¸å¹³è¡¡é—®é¢˜")
    
    print("\n" + "="*80)

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    data_dir = "./MS_MultiSpine_dataset/MS_MultiSpine_dataset/derivatives/preprocessedAndRegistered"
    
    print_optimization_summary()
    
    # åˆ›å»ºä¼˜åŒ–é…ç½®
    config = create_optimized_training_config(
        data_dir=data_dir,
        total_epochs=10,
        device="cpu"
    )
    
    print("\nâœ… ä¼˜åŒ–ç­–ç•¥è„šæœ¬åˆ›å»ºå®Œæˆï¼")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. å¯¼å…¥æ­¤æ¨¡å—: from MSMultiSpineLoader import create_optimized_training_config")
    print("2. åˆ›å»ºé…ç½®: config = create_optimized_training_config(data_dir, total_epochs, device)")
    print("3. åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨config['transforms']['train'](epoch)è·å–å˜æ¢")
    print("4. ä½¿ç”¨config['loss_function']ä½œä¸ºæŸå¤±å‡½æ•°")
    print("5. å‚è€ƒconfig['training_params']è°ƒæ•´è®­ç»ƒå‚æ•°")