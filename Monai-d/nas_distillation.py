import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any
import logging
import os
import json
import time
from pathlib import Path
from torch.cuda.amp import GradScaler
from torch.amp import autocast
import gc
from tqdm import tqdm
from colorama import init, Fore, Back, Style
import psutil

from nas_search import SuperNet, DARTSSearcher
from knowledge_distillation import KnowledgeDistillationLoss, MultiTeacherDistillation
from model import ModelFactory

# åˆå§‹åŒ–coloramaç”¨äºå½©è‰²ç»ˆç«¯è¾“å‡º
init(autoreset=True)

class NASDistillationIntegration:
    """
    NAS-è’¸é¦é›†æˆç±»ï¼šå®ç°ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹
    é˜¶æ®µ1ï¼šNASæœç´¢æ‰¾åˆ°æœ€ä¼˜æ¶æ„
    é˜¶æ®µ2ï¼šä½¿ç”¨æœ€ä¼˜æ¶æ„ä½œä¸ºå­¦ç”Ÿæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦è®­ç»ƒ
    """
    
    def __init__(self,
                 teacher_models: List[str],
                 device: torch.device,
                 dataset_type: str = 'BraTS',
                 nas_epochs: int = 50,
                 distillation_epochs: int = 100,
                 arch_lr: float = 3e-4,
                 model_lr: float = 1e-3,
                 distillation_lr: float = 1e-4,
                 temperature: float = 4.0,
                 alpha: float = 0.7,
                 save_dir: str = './checkpoints/nas_distillation',
                 # NASç›¸å…³å‚æ•°
                 nas_type: str = 'darts',
                 base_channels: int = 8,
                 num_layers: int = 3,
                 max_layers: int = 8,
                 start_layers: int = 1,
                 # çŸ¥è¯†è’¸é¦å‚æ•°æ˜ å°„
                 distillation_temperature: Optional[float] = None,
                 distillation_alpha: Optional[float] = None,
                 # æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒå‚æ•°
                 teacher_pretrain_epochs: int = 50,
                 teacher_learning_rate: float = 1e-4,
                 # å…¶ä»–å‚æ•°
                 auto_adjust: bool = False):
        """
        åˆå§‹åŒ–NAS-è’¸é¦é›†æˆå™¨
        
        Args:
            teacher_models: æ•™å¸ˆæ¨¡å‹åˆ—è¡¨
            device: è®¡ç®—è®¾å¤‡
            dataset_type: æ•°æ®é›†ç±»å‹
            nas_epochs: NASæœç´¢è½®æ•°
            distillation_epochs: çŸ¥è¯†è’¸é¦è½®æ•°
            arch_lr: æ¶æ„å‚æ•°å­¦ä¹ ç‡
            model_lr: æ¨¡å‹å‚æ•°å­¦ä¹ ç‡
            distillation_lr: è’¸é¦é˜¶æ®µå­¦ä¹ ç‡
            temperature: è’¸é¦æ¸©åº¦
            alpha: è’¸é¦æŸå¤±æƒé‡
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        self.teacher_models = teacher_models
        self.device = device
        self.dataset_type = dataset_type
        self.nas_epochs = nas_epochs
        self.distillation_epochs = distillation_epochs
        self.arch_lr = arch_lr
        self.model_lr = model_lr
        self.distillation_lr = distillation_lr
        
        # çŸ¥è¯†è’¸é¦å‚æ•°æ˜ å°„å¤„ç†
        self.temperature = distillation_temperature if distillation_temperature is not None else temperature
        self.alpha = distillation_alpha if distillation_alpha is not None else alpha
        
        # NASç›¸å…³å‚æ•°
        self.nas_type = nas_type
        self.base_channels = base_channels
        self.num_layers = num_layers
        self.max_layers = max_layers
        self.start_layers = start_layers
        self.auto_adjust = auto_adjust
        
        # æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒå‚æ•°
        self.teacher_pretrain_epochs = teacher_pretrain_epochs
        self.teacher_learning_rate = teacher_learning_rate
        
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ•°æ®é›†ç›¸å…³å‚æ•°
        if dataset_type == 'MS_MultiSpine':
            self.in_channels = 2
            self.num_classes = 6
        else:  # BraTS
            self.in_channels = 4
            self.num_classes = 4
            
        # åˆå§‹åŒ–ç»„ä»¶
        self.supernet = None
        self.searcher = None
        self.teacher_ensemble = None
        self.student_model = None
        self.distillation_model = None
        
        # è®­ç»ƒçŠ¶æ€
        self.nas_completed = False
        self.best_architecture = None
        
        # å†…å­˜ä¼˜åŒ–è®¾ç½®
        self._setup_memory_optimization()
        
        # æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®
        self.use_amp = torch.cuda.is_available()
        self.scaler = GradScaler() if self.use_amp else None
        
        # å†…å­˜ç›‘æ§è®¾ç½®
        self.memory_monitor_interval = 10  # æ¯10ä¸ªbatchç›‘æ§ä¸€æ¬¡å†…å­˜
        self.auto_cleanup_threshold = 8.0  # å†…å­˜ä½¿ç”¨è¶…è¿‡8GBæ—¶è‡ªåŠ¨æ¸…ç†
        
        logging.info(f"åˆå§‹åŒ–NAS-è’¸é¦é›†æˆå™¨: æ•™å¸ˆæ¨¡å‹={teacher_models}, æ•°æ®é›†={dataset_type}")
        logging.info(f"æ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if self.use_amp else 'ç¦ç”¨'}")
        logging.info(f"å†…å­˜ç›‘æ§: é—´éš”={self.memory_monitor_interval}batch, è‡ªåŠ¨æ¸…ç†é˜ˆå€¼={self.auto_cleanup_threshold}GB")
        
        # è®­ç»ƒè¿›åº¦è·Ÿè¸ª
        self.training_start_time = None
        self.current_phase = "åˆå§‹åŒ–"
        self.phase_start_time = None
        
    def _setup_memory_optimization(self):
        """
        è®¾ç½®å†…å­˜ä¼˜åŒ–é…ç½®
        """
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
        if torch.cuda.is_available():
            # å¯ç”¨å¯æ‰©å±•æ®µä»¥å‡å°‘å†…å­˜ç¢ç‰‡
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'
            
            # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.empty_cache()
            
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥ä»¥å‡å°‘ç¢ç‰‡
            torch.cuda.set_per_process_memory_fraction(0.85)  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨ä¸º85%
            
            # å¯ç”¨å†…å­˜å†å²è®°å½•ç”¨äºè°ƒè¯•
            try:
                torch.cuda.memory._record_memory_history(enabled=True, alloc_trace_record_context=True)
            except:
                pass
            
            logging.info("[å†…å­˜ä¼˜åŒ–] å·²å¯ç”¨CUDAå†…å­˜ä¼˜åŒ–è®¾ç½®")
            logging.info(f"[å†…å­˜ä¼˜åŒ–] GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            logging.info("[å†…å­˜ä¼˜åŒ–] å·²è®¾ç½®GPUå†…å­˜ä½¿ç”¨é™åˆ¶ä¸º85%")
            logging.info("[å†…å­˜ä¼˜åŒ–] å·²å¯ç”¨å†…å­˜å†å²è®°å½•å’Œè‡ªåŠ¨æ¸…ç†æœºåˆ¶")
        
    def _clear_memory_cache(self):
        """
        å¼ºåŒ–çš„GPUå†…å­˜ç¼“å­˜æ¸…ç†
        """
        # å¤šæ¬¡å¼ºåˆ¶åƒåœ¾å›æ”¶
        for _ in range(3):
            gc.collect()
        
        if torch.cuda.is_available():
            # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # å°è¯•å†…å­˜ç¢ç‰‡æ•´ç†
            try:
                torch.cuda.memory._dump_snapshot("memory_snapshot.pickle")
                torch.cuda.memory._record_memory_history(enabled=None)
            except:
                pass
            
            # è®°å½•è¯¦ç»†å†…å­˜ä½¿ç”¨æƒ…å†µ
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3    # GB
            max_allocated = torch.cuda.max_memory_allocated() / 1024**3  # GB
            max_reserved = torch.cuda.max_memory_reserved() / 1024**3    # GB
            
            logging.info(f"GPUå†…å­˜çŠ¶æ€: å½“å‰åˆ†é… {allocated:.2f}GB, å½“å‰ä¿ç•™ {reserved:.2f}GB")
            logging.info(f"GPUå†…å­˜å³°å€¼: æœ€å¤§åˆ†é… {max_allocated:.2f}GB, æœ€å¤§ä¿ç•™ {max_reserved:.2f}GB")
            
            # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå‘å‡ºè­¦å‘Š
            if allocated > 10.0:  # è¶…è¿‡10GB
                logging.warning(f"GPUå†…å­˜ä½¿ç”¨è¿‡é«˜: {allocated:.2f}GBï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")
            
            # é‡ç½®å†…å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
        
    def initialize_nas_search(self) -> None:
        """
        åˆå§‹åŒ–NASæœç´¢ç»„ä»¶
        """
        logging.info("åˆå§‹åŒ–NASæœç´¢ç»„ä»¶...")
        
        # åˆ›å»ºè¶…ç½‘ç»œï¼ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼‰
        self.supernet = SuperNet(
            in_channels=self.in_channels,
            num_classes=self.num_classes,
            base_channels=self.base_channels,
            num_layers=self.num_layers,
            dataset_type=self.dataset_type
        )
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        if hasattr(self.supernet, 'enable_checkpointing'):
            self.supernet.enable_checkpointing()
        
        # ä¸ºæ‰€æœ‰æ¨¡å—å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        for module in self.supernet.modules():
            if hasattr(module, 'gradient_checkpointing'):
                module.gradient_checkpointing = True
        
        # æ¸…ç†å†…å­˜ç¼“å­˜
        self._clear_memory_cache()
        
        # åˆ›å»ºDARTSæœç´¢å™¨
        self.searcher = DARTSSearcher(
            supernet=self.supernet,
            device=self.device,
            arch_lr=self.arch_lr,
            model_lr=self.model_lr
        )
        
        logging.info("NASæœç´¢ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        
    def initialize_teacher_models(self) -> None:
        """
        åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹é›†åˆ
        """
        logging.info("åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹é›†åˆ...")
        
        # åˆ›å»ºæ¨¡å‹å·¥å‚
        model_factory = ModelFactory()
        
        # åŠ è½½æ•™å¸ˆæ¨¡å‹
        teachers = {}
        for model_name in self.teacher_models:
            try:
                model_config = {
                    'category': 'basic',
                    'model_name': model_name,
                    'device': self.device,
                    'dataset_type': self.dataset_type
                }
                model_bank = model_factory.create_model(model_config)
                model = model_bank.model
                model = model.to(self.device)
                model.eval()  # æ•™å¸ˆæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼
                teachers[model_name] = model
                logging.info(f"æˆåŠŸåŠ è½½æ•™å¸ˆæ¨¡å‹: {model_name}")
            except Exception as e:
                logging.warning(f"åŠ è½½æ•™å¸ˆæ¨¡å‹ {model_name} å¤±è´¥: {e}")
                
        if not teachers:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•™å¸ˆæ¨¡å‹")
            
        self.teacher_ensemble = teachers
        logging.info(f"æ•™å¸ˆæ¨¡å‹é›†åˆåˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {len(teachers)} ä¸ªæ¨¡å‹")
        
    def pretrain_teacher_models(self, train_loader, val_loader) -> Dict[str, Any]:
        """
        é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            é¢„è®­ç»ƒç»“æœ
        """
        # æ‰“å°é˜¶æ®µæ ‡é¢˜
        self._print_phase_header(
            "æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒé˜¶æ®µ", 
            f"æ•™å¸ˆæ¨¡å‹: {', '.join(self.teacher_models)} | æ£€æŸ¥é¢„è®­ç»ƒæƒé‡"
        )
        
        pretrain_results = {
            'pretrained_models': [],
            'skipped_models': [],
            'individual_results': {},
            'total_time': 0
        }
        
        pretrain_start_time = time.time()
        
        # åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
        if not hasattr(self, 'teacher_ensemble') or not self.teacher_ensemble:
            self.initialize_teacher_models()
        
        # æ£€æŸ¥æ¯ä¸ªæ•™å¸ˆæ¨¡å‹æ˜¯å¦éœ€è¦é¢„è®­ç»ƒ
        models_to_pretrain = []
        for model_name in self.teacher_models:
            pretrained_path = self.save_dir.parent / 'pretrained_teachers' / model_name / 'models' / 'best_model.pth'
            if not pretrained_path.exists():
                models_to_pretrain.append(model_name)
                print(f"{Fore.YELLOW}âš ï¸  æ•™å¸ˆæ¨¡å‹ {model_name} éœ€è¦é¢„è®­ç»ƒ{Style.RESET_ALL}")
            else:
                pretrain_results['skipped_models'].append(model_name)
                print(f"{Fore.GREEN}âœ… æ•™å¸ˆæ¨¡å‹ {model_name} å·²æœ‰é¢„è®­ç»ƒæƒé‡{Style.RESET_ALL}")
        
        if not models_to_pretrain:
            print(f"{Fore.GREEN}âœ… æ‰€æœ‰æ•™å¸ˆæ¨¡å‹éƒ½å·²æœ‰é¢„è®­ç»ƒæƒé‡ï¼Œè·³è¿‡é¢„è®­ç»ƒé˜¶æ®µ!{Style.RESET_ALL}")
        else:
            print(f"{Fore.CYAN}ğŸš€ å¼€å§‹é¢„è®­ç»ƒ {len(models_to_pretrain)} ä¸ªæ•™å¸ˆæ¨¡å‹...{Style.RESET_ALL}")
            
            # ä¸ºæ¯ä¸ªéœ€è¦é¢„è®­ç»ƒçš„æ¨¡å‹æ‰§è¡Œé¢„è®­ç»ƒ
            for idx, model_name in enumerate(models_to_pretrain, 1):
                model_start_time = time.time()
                
                print(f"\n{Fore.MAGENTA}ğŸ“ æ•™å¸ˆæ¨¡å‹ {idx}/{len(models_to_pretrain)}: {model_name}{Style.RESET_ALL}")
                print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}")
                
                # æ‰§è¡Œå•ä¸ªæ•™å¸ˆæ¨¡å‹çš„é¢„è®­ç»ƒ
                model_result = self._pretrain_single_teacher(
                    model_name, train_loader, val_loader, idx, len(models_to_pretrain)
                )
                
                pretrain_results['individual_results'][model_name] = model_result
                pretrain_results['pretrained_models'].append(model_name)
                
                model_time = time.time() - model_start_time
                print(f"{Fore.GREEN}âœ… æ•™å¸ˆæ¨¡å‹ {model_name} é¢„è®­ç»ƒå®Œæˆ! è€—æ—¶: {self._format_time(model_time)}{Style.RESET_ALL}")
                print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - pretrain_start_time
        pretrain_results['total_time'] = total_time
        
        print(f"{Fore.GREEN}ğŸ‰ æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒé˜¶æ®µå®Œæˆ!{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“Š é¢„è®­ç»ƒç»Ÿè®¡:{Style.RESET_ALL}")
        print(f"   â€¢ æ•™å¸ˆæ¨¡å‹æ€»æ•°: {len(self.teacher_models)}")
        print(f"   â€¢ é¢„è®­ç»ƒæ¨¡å‹: {len(pretrain_results['pretrained_models'])}")
        print(f"   â€¢ è·³è¿‡æ¨¡å‹: {len(pretrain_results['skipped_models'])}")
        print(f"   â€¢ æ€»è€—æ—¶: {self._format_time(total_time)}")
        print(f"   â€¢ å†…å­˜ä½¿ç”¨: {self._get_memory_info_str()}")
        
        if pretrain_results['individual_results']:
            print(f"   â€¢ å„æ¨¡å‹è¯¦æƒ…:")
            for model_name, result in pretrain_results['individual_results'].items():
                print(f"     - {model_name}: æœ€ä½³æŸå¤± {result['best_val_loss']:.4f}, è€—æ—¶ {self._format_time(result['training_time'])}")
        
        return pretrain_results
    
    def _pretrain_single_teacher(self, model_name: str, train_loader, val_loader, 
                                model_idx: int, total_models: int) -> Dict[str, Any]:
        """
        é¢„è®­ç»ƒå•ä¸ªæ•™å¸ˆæ¨¡å‹
        
        Args:
            model_name: æ•™å¸ˆæ¨¡å‹åç§°
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            model_idx: å½“å‰æ¨¡å‹ç´¢å¼•
            total_models: æ€»æ¨¡å‹æ•°é‡
            
        Returns:
            å•ä¸ªæ¨¡å‹çš„é¢„è®­ç»ƒç»“æœ
        """
        from torch.optim import Adam
        from torch.optim.lr_scheduler import ReduceLROnPlateau
        import torch.nn.functional as F
        
        # è·å–æ•™å¸ˆæ¨¡å‹
        teacher_model = self.teacher_ensemble[model_name]
        teacher_model.train()
        
        # è®¾ç½®é¢„è®­ç»ƒå‚æ•°
        pretrain_epochs = getattr(self, 'teacher_pretrain_epochs', 50)  # é»˜è®¤50è½®
        learning_rate = getattr(self, 'teacher_learning_rate', 1e-4)  # é»˜è®¤å­¦ä¹ ç‡
        
        print(f"{Fore.WHITE}é…ç½®: è½®æ•°={pretrain_epochs}, å­¦ä¹ ç‡={learning_rate}, è®¾å¤‡={self.device}{Style.RESET_ALL}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = Adam(teacher_model.parameters(), lr=learning_rate)
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)
        
        # è®­ç»ƒç»Ÿè®¡
        best_val_loss = float('inf')
        training_stats = {
            'train_losses': [],
            'val_losses': [],
            'learning_rates': [],
            'best_epoch': 0,
            'training_time': 0
        }
        
        model_start_time = time.time()
        
        # åˆ›å»ºä¸»è¿›åº¦æ¡
        epoch_pbar = tqdm(
            range(pretrain_epochs),
            desc=f"{Fore.BLUE}ğŸ“ {model_name} é¢„è®­ç»ƒ{Style.RESET_ALL}",
            ncols=120,
            leave=True,
            position=0,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}"
        )
        
        for epoch in epoch_pbar:
            epoch_start_time = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            teacher_model.train()
            train_loss = 0.0
            train_batches = 0
            
            try:
                for batch_idx, batch in enumerate(train_loader):
                    data, target = batch['image'].to(self.device), batch['label'].to(self.device)
                    
                    # å¤„ç†ç›®æ ‡å¼ é‡ç»´åº¦
                    if target.dim() == 5 and target.size(1) == 1:
                        target = target.squeeze(1)
                    target = target.long()
                    
                    # éªŒè¯targetå¼ é‡å€¼èŒƒå›´
                    target_min, target_max = target.min().item(), target.max().item()
                    if target_min < 0 or target_max >= self.num_classes:
                        target = torch.clamp(target, 0, self.num_classes - 1)
                    
                    # å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
                    optimizer.zero_grad()
                    
                    if self.use_amp:
                        with autocast('cuda'):
                            output = teacher_model(data)
                            loss = F.cross_entropy(output, target)
                        
                        self.scaler.scale(loss).backward()
                        self.scaler.step(optimizer)
                        self.scaler.update()
                    else:
                        output = teacher_model(data)
                        loss = F.cross_entropy(output, target)
                        loss.backward()
                        optimizer.step()
                    
                    train_loss += loss.item()
                    train_batches += 1
                    
                    # å†…å­˜ç›‘æ§
                    if batch_idx % self.memory_monitor_interval == 0:
                        self._monitor_and_cleanup_memory()
                        
            except RuntimeError as e:
                if "DataLoader worker" in str(e):
                    logging.error(f"æ•™å¸ˆæ¨¡å‹ {model_name} è®­ç»ƒDataLoaderé”™è¯¯: {e}")
                    train_loader = self._recreate_dataloader(train_loader, num_workers=0)
                    continue
                else:
                    raise e
            
            # éªŒè¯é˜¶æ®µ
            teacher_model.eval()
            val_loss = 0.0
            val_batches = 0
            
            with torch.no_grad():
                try:
                    for batch in val_loader:
                        data, target = batch['image'].to(self.device), batch['label'].to(self.device)
                        
                        # å¤„ç†ç›®æ ‡å¼ é‡ç»´åº¦
                        if target.dim() == 5 and target.size(1) == 1:
                            target = target.squeeze(1)
                        target = target.long()
                        
                        # éªŒè¯targetå¼ é‡å€¼èŒƒå›´
                        target_min, target_max = target.min().item(), target.max().item()
                        if target_min < 0 or target_max >= self.num_classes:
                            target = torch.clamp(target, 0, self.num_classes - 1)
                        
                        if self.use_amp:
                            with autocast('cuda'):
                                output = teacher_model(data)
                                batch_loss = F.cross_entropy(output, target).item()
                        else:
                            output = teacher_model(data)
                            batch_loss = F.cross_entropy(output, target).item()
                        
                        val_loss += batch_loss
                        val_batches += 1
                        
                except RuntimeError as e:
                    if "DataLoader worker" in str(e):
                        logging.error(f"æ•™å¸ˆæ¨¡å‹ {model_name} éªŒè¯DataLoaderé”™è¯¯: {e}")
                        val_loader = self._recreate_dataloader(val_loader, num_workers=0)
                        continue
                    else:
                        raise e
            
            # è®¡ç®—å¹³å‡æŸå¤±
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            epoch_time = time.time() - epoch_start_time
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            training_stats['train_losses'].append(train_loss)
            training_stats['val_losses'].append(val_loss)
            training_stats['learning_rates'].append(optimizer.param_groups[0]['lr'])
            
            # æ›´æ–°ä¸»è¿›åº¦æ¡
            epoch_pbar.set_postfix({
                'Train_Loss': f'{train_loss:.4f}',
                'Val_Loss': f'{val_loss:.4f}',
                'Best': f'{best_val_loss:.4f}',
                'LR': f'{optimizer.param_groups[0]["lr"]:.6f}',
                'Time': f'{epoch_time:.1f}s',
                'Mem': self._get_memory_info_str().split('|')[0].strip()
            })
            
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                training_stats['best_epoch'] = epoch
                
                # æ˜¾ç¤ºæ–°çš„æœ€ä½³ç»“æœ
                tqdm.write(f"{Fore.GREEN}âœ¨ {model_name} æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.4f} (Epoch {epoch+1}){Style.RESET_ALL}")
                
                # ä¿å­˜æœ€ä¼˜æ•™å¸ˆæ¨¡å‹
                teacher_save_dir = self.save_dir.parent / 'pretrained_teachers' / model_name / 'models'
                teacher_save_dir.mkdir(parents=True, exist_ok=True)
                teacher_save_path = teacher_save_dir / 'best_model.pth'
                
                torch.save({
                    'model_state_dict': teacher_model.state_dict(),
                    'val_loss': val_loss,
                    'epoch': epoch,
                    'model_name': model_name,
                    'dataset_type': self.dataset_type,
                    'training_config': {
                        'learning_rate': learning_rate,
                        'epochs': pretrain_epochs,
                        'optimizer': 'Adam'
                    }
                }, teacher_save_path)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            
            # æ¯ä¸ªepochç»“æŸåæ¸…ç†å†…å­˜ç¼“å­˜
            self._clear_memory_cache()
        
        # å…³é—­è¿›åº¦æ¡
        epoch_pbar.close()
        
        # è®°å½•è®­ç»ƒæ—¶é—´
        training_stats['training_time'] = time.time() - model_start_time
        training_stats['best_val_loss'] = best_val_loss
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        stats_save_dir = self.save_dir.parent / 'pretrained_teachers' / model_name
        stats_save_dir.mkdir(parents=True, exist_ok=True)
        stats_save_path = stats_save_dir / 'training_stats.json'
        
        with open(stats_save_path, 'w') as f:
            json.dump(training_stats, f, indent=2)
        
        return training_stats
    
    def _format_time(self, seconds: float) -> str:
        """
        æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
        
        Args:
            seconds: ç§’æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"
    
    def search_architecture(self, train_loader, val_loader) -> Dict[str, Any]:
        """
        æ‰§è¡ŒNASæ¶æ„æœç´¢
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            æœ€ä¼˜æ¶æ„ä¿¡æ¯
        """
        if not self.supernet or not self.searcher:
            self.initialize_nas_search()
        
        # æ‰“å°é˜¶æ®µæ ‡é¢˜
        self._print_phase_header(
            "NASæ¶æ„æœç´¢é˜¶æ®µ", 
            f"æœç´¢è½®æ•°: {self.nas_epochs} | æ¶æ„å­¦ä¹ ç‡: {self.arch_lr} | æ¨¡å‹å­¦ä¹ ç‡: {self.model_lr}"
        )
        
        best_val_loss = float('inf')
        best_arch_params = None
        search_start_time = time.time()
        
        # åˆ›å»ºä¸»è¿›åº¦æ¡
        epoch_pbar = tqdm(
            range(self.nas_epochs),
            desc=f"{Fore.GREEN}ğŸ” NASæœç´¢{Style.RESET_ALL}",
            ncols=120,
            leave=True,
            position=0,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}"
        )
        
        for epoch in epoch_pbar:
            epoch_start_time = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            self.supernet.train()
            train_loss = 0.0
            train_batches = 0
            
            # åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡è¿›åº¦æ¡
            train_pbar = tqdm(
                enumerate(train_loader),
                total=len(train_loader),
                desc=f"{Fore.BLUE}  ğŸ“š è®­ç»ƒ{Style.RESET_ALL}",
                leave=False,
                position=1,
                ncols=100,
                disable=len(train_loader) < 10  # å¦‚æœbatchæ•°é‡å¤ªå°‘ï¼Œç¦ç”¨å­è¿›åº¦æ¡
            )
            
            try:
                for batch_idx, batch in train_pbar:
                    data, target = batch['image'].to(self.device), batch['label'].to(self.device)
                    
                    # å¤„ç†ç›®æ ‡å¼ é‡ç»´åº¦ï¼šç§»é™¤å¤šä½™çš„é€šé“ç»´åº¦
                    if target.dim() == 5 and target.size(1) == 1:
                        target = target.squeeze(1)  # ä»[B, 1, H, W, D]å˜ä¸º[B, H, W, D]
                    
                    # ç¡®ä¿ç›®æ ‡å¼ é‡ä¸ºLongç±»å‹ï¼ˆäº¤å‰ç†µæŸå¤±è¦æ±‚ï¼‰
                    target = target.long()
                    
                    # éªŒè¯targetå¼ é‡å€¼èŒƒå›´ï¼Œé˜²æ­¢CUDAè®¾å¤‡ç«¯æ–­è¨€é”™è¯¯
                    target_min, target_max = target.min().item(), target.max().item()
                    if target_min < 0 or target_max >= self.num_classes:
                        logging.warning(f"Targetå¼ é‡å€¼è¶…å‡ºèŒƒå›´: min={target_min}, max={target_max}, num_classes={self.num_classes}")
                        # å°†targetå€¼é™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        target = torch.clamp(target, 0, self.num_classes - 1)
                    
                    # æ£€æŸ¥å¹¶å¤„ç†NaNæˆ–æ— ç©·å€¼
                    if torch.isnan(target).any() or torch.isinf(target).any():
                        logging.error("Targetå¼ é‡åŒ…å«NaNæˆ–æ— ç©·å€¼ï¼Œè·³è¿‡æ­¤batch")
                        continue
                    
                    # æ›´æ–°æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼‰
                    self.searcher.model_optimizer.zero_grad()
                    
                    if self.use_amp:
                        with autocast('cuda'):
                            output = self.supernet(data)
                            loss = F.cross_entropy(output, target)
                        
                        self.scaler.scale(loss).backward()
                        self.scaler.step(self.searcher.model_optimizer)
                        self.scaler.update()
                    else:
                        output = self.supernet(data)
                        loss = F.cross_entropy(output, target)
                        loss.backward()
                        self.searcher.model_optimizer.step()
                    # æ›´æ–°æ¶æ„å‚æ•°ï¼ˆæ¯éš”å‡ ä¸ªbatchï¼Œä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼‰
                    if batch_idx % 5 == 0:
                        self.searcher.arch_optimizer.zero_grad()
                        
                        if self.use_amp:
                            with autocast('cuda'):
                                arch_output = self.supernet(data)
                                arch_loss = F.cross_entropy(arch_output, target)
                            
                            self.scaler.scale(arch_loss).backward()
                            self.scaler.step(self.searcher.arch_optimizer)
                            self.scaler.update()
                        else:
                            arch_output = self.supernet(data)
                            arch_loss = F.cross_entropy(arch_output, target)
                            arch_loss.backward()
                            self.searcher.arch_optimizer.step()
                    train_loss += loss.item()
                    train_batches += 1
                    
                    # æ›´æ–°è®­ç»ƒè¿›åº¦æ¡
                    train_pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Avg': f'{train_loss/train_batches:.4f}',
                        'Mem': self._get_memory_info_str().split('|')[0].strip()
                    })
                    
                    # å†…å­˜ç›‘æ§å’Œè‡ªåŠ¨æ¸…ç†
                    if batch_idx % self.memory_monitor_interval == 0:
                        self._monitor_and_cleanup_memory()
                        
            except RuntimeError as e:
                if "DataLoader worker" in str(e):
                    logging.error(f"DataLoader workeré”™è¯¯: {e}")
                    logging.info("å°è¯•é‡æ–°åˆ›å»ºDataLoader...")
                    # é‡æ–°åˆ›å»ºtrain_loaderï¼Œå¼ºåˆ¶è®¾ç½®num_workers=0
                    train_loader = self._recreate_dataloader(train_loader, num_workers=0)
                    continue
                else:
                    raise e
            
            # éªŒè¯é˜¶æ®µ
            self.supernet.eval()
            val_loss = 0.0
            val_batches = 0
            
            # åˆ›å»ºéªŒè¯æ‰¹æ¬¡è¿›åº¦æ¡
            val_pbar = tqdm(
                val_loader,
                desc=f"{Fore.MAGENTA}  ğŸ” éªŒè¯{Style.RESET_ALL}",
                leave=False,
                position=1,
                ncols=100,
                disable=len(val_loader) < 10  # å¦‚æœbatchæ•°é‡å¤ªå°‘ï¼Œç¦ç”¨å­è¿›åº¦æ¡
            )
            
            with torch.no_grad():
                try:
                    for batch in val_pbar:
                        data, target = batch['image'].to(self.device), batch['label'].to(self.device)
                        
                        # å¤„ç†ç›®æ ‡å¼ é‡ç»´åº¦ï¼šç§»é™¤å¤šä½™çš„é€šé“ç»´åº¦
                        if target.dim() == 5 and target.size(1) == 1:
                            target = target.squeeze(1)  # ä»[B, 1, H, W, D]å˜ä¸º[B, H, W, D]
                        
                        # ç¡®ä¿ç›®æ ‡å¼ é‡ä¸ºLongç±»å‹ï¼ˆäº¤å‰ç†µæŸå¤±è¦æ±‚ï¼‰
                        target = target.long()
                        
                        # éªŒè¯targetå¼ é‡å€¼èŒƒå›´ï¼Œé˜²æ­¢CUDAè®¾å¤‡ç«¯æ–­è¨€é”™è¯¯
                        target_min, target_max = target.min().item(), target.max().item()
                        if target_min < 0 or target_max >= self.num_classes:
                            logging.warning(f"éªŒè¯é˜¶æ®µTargetå¼ é‡å€¼è¶…å‡ºèŒƒå›´: min={target_min}, max={target_max}, num_classes={self.num_classes}")
                            # å°†targetå€¼é™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´å†…
                            target = torch.clamp(target, 0, self.num_classes - 1)
                        
                        # æ£€æŸ¥å¹¶å¤„ç†NaNæˆ–æ— ç©·å€¼
                        if torch.isnan(target).any() or torch.isinf(target).any():
                            logging.error("éªŒè¯é˜¶æ®µTargetå¼ é‡åŒ…å«NaNæˆ–æ— ç©·å€¼ï¼Œè·³è¿‡æ­¤batch")
                            continue
                        
                        if self.use_amp:
                            with autocast('cuda'):
                                output = self.supernet(data)
                                batch_loss = F.cross_entropy(output, target).item()
                        else:
                            output = self.supernet(data)
                            batch_loss = F.cross_entropy(output, target).item()
                        
                        val_loss += batch_loss
                        val_batches += 1
                        
                        # æ›´æ–°éªŒè¯è¿›åº¦æ¡
                        val_pbar.set_postfix({
                            'Loss': f'{batch_loss:.4f}',
                            'Avg': f'{val_loss/val_batches:.4f}'
                        })
                except RuntimeError as e:
                    if "DataLoader worker" in str(e):
                        logging.error(f"éªŒè¯é˜¶æ®µDataLoader workeré”™è¯¯: {e}")
                        logging.info("å°è¯•é‡æ–°åˆ›å»ºéªŒè¯DataLoader...")
                        val_loader = self._recreate_dataloader(val_loader, num_workers=0)
                        continue
                    else:
                        raise e
            
            val_loss /= len(val_loader)
            epoch_time = time.time() - epoch_start_time
            
            # æ›´æ–°ä¸»è¿›åº¦æ¡
            epoch_pbar.set_postfix({
                'Train_Loss': f'{train_loss/train_batches:.4f}',
                'Val_Loss': f'{val_loss:.4f}',
                'Best': f'{best_val_loss:.4f}',
                'Time': f'{epoch_time:.1f}s',
                'ETA': self._estimate_remaining_time(epoch + 1, self.nas_epochs, search_start_time)
            })
            
            # ä¿å­˜æœ€ä¼˜æ¶æ„
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_arch_params = {name: param.clone() for name, param in self.supernet.arch_parameters.items()}
                
                # æ˜¾ç¤ºæ–°çš„æœ€ä½³ç»“æœ
                tqdm.write(f"{Fore.GREEN}âœ¨ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.4f} (Epoch {epoch+1}){Style.RESET_ALL}")
                
                # ä¿å­˜æœ€ä¼˜æ¶æ„
                arch_save_path = self.save_dir / 'best_architecture.pth'
                torch.save({
                    'arch_parameters': best_arch_params,
                    'val_loss': best_val_loss,
                    'epoch': epoch
                }, arch_save_path)
            
            # æ¯ä¸ªepochç»“æŸåæ¸…ç†å†…å­˜ç¼“å­˜
            self._clear_memory_cache()
        
        # å…³é—­è¿›åº¦æ¡
        epoch_pbar.close()
        
        # æœç´¢å®Œæˆæ€»ç»“
        search_time = time.time() - search_start_time
        print(f"\n{Fore.GREEN}ğŸ‰ NASæ¶æ„æœç´¢å®Œæˆ!{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“Š æœç´¢ç»Ÿè®¡:{Style.RESET_ALL}")
        print(f"   â€¢ æ€»è€—æ—¶: {self._get_elapsed_time_str(search_start_time)}")
        print(f"   â€¢ æœ€ä½³éªŒè¯æŸå¤±: {Fore.YELLOW}{best_val_loss:.4f}{Style.RESET_ALL}")
        print(f"   â€¢ å†…å­˜ä½¿ç”¨: {self._get_memory_info_str()}")
        print(f"   â€¢ æ¶æ„ä¿å­˜è‡³: {self.save_dir / 'best_architecture.pth'}")
        
        # ä¿å­˜æœç´¢ç»“æœ
        self.best_architecture = {
            'arch_parameters': best_arch_params,
            'val_loss': best_val_loss,
            'supernet_state': self.supernet.state_dict()
        }
        
        self.nas_completed = True
        
        return self.best_architecture
        
    def create_student_model(self) -> nn.Module:
        """
        åŸºäºæœç´¢åˆ°çš„æœ€ä¼˜æ¶æ„åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
        
        Returns:
            å­¦ç”Ÿæ¨¡å‹
        """
        if not self.nas_completed or not self.best_architecture:
            raise ValueError("å¿…é¡»å…ˆå®ŒæˆNASæœç´¢æ‰èƒ½åˆ›å»ºå­¦ç”Ÿæ¨¡å‹")
            
        logging.info("åŸºäºæœ€ä¼˜æ¶æ„åˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
        
        # åˆ›å»ºæ–°çš„SuperNetå®ä¾‹ä½œä¸ºå­¦ç”Ÿæ¨¡å‹
        student = SuperNet(
            in_channels=self.in_channels,
            num_classes=self.num_classes,
            base_channels=8,  # å¤§å¹…å‡å°‘base_channelsä»¥èŠ‚çœå†…å­˜
            num_layers=3,  # å‡å°‘å±‚æ•°
            dataset_type=self.dataset_type
        )
        
        # åŠ è½½æœ€ä¼˜æ¶æ„å‚æ•°
        student.arch_parameters.load_state_dict(self.best_architecture['arch_parameters'])
        
        # å†»ç»“æ¶æ„å‚æ•°ï¼Œåªè®­ç»ƒæ¨¡å‹å‚æ•°
        for param in student.arch_parameters.parameters():
            param.requires_grad = False
            
        self.student_model = student.to(self.device)
        logging.info("å­¦ç”Ÿæ¨¡å‹åˆ›å»ºå®Œæˆ")
        
        return self.student_model
        
    def initialize_distillation(self) -> None:
        """
        åˆå§‹åŒ–çŸ¥è¯†è’¸é¦ç»„ä»¶
        """
        if not self.teacher_ensemble:
            self.initialize_teacher_models()
            
        if not self.student_model:
            self.create_student_model()
            
        logging.info("åˆå§‹åŒ–çŸ¥è¯†è’¸é¦ç»„ä»¶...")
        
        # åˆ›å»ºå¤šæ•™å¸ˆè’¸é¦æ¨¡å‹
        teacher_model_list = list(self.teacher_ensemble.values())  # ä»å­—å…¸ä¸­æå–æ¨¡å‹å¯¹è±¡åˆ—è¡¨
        self.distillation_model = MultiTeacherDistillation(
            teacher_models=teacher_model_list,
            student_model=self.student_model,
            device=self.device,
            temperature=self.temperature
        )
        
        self.distillation_model = self.distillation_model.to(self.device)
        logging.info("çŸ¥è¯†è’¸é¦ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        
    def _recreate_dataloader(self, original_loader, num_workers=0):
        """
        é‡æ–°åˆ›å»ºDataLoaderï¼Œå¼ºåˆ¶è®¾ç½®num_workers=0å’ŒWindowså…¼å®¹æ€§è®¾ç½®
        
        Args:
            original_loader: åŸå§‹DataLoader
            num_workers: workerè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º0
            
        Returns:
            æ–°çš„DataLoader
        """
        from monai.data import DataLoader as MonaiDataLoader
        
        new_loader = MonaiDataLoader(
            original_loader.dataset,
            batch_size=original_loader.batch_size,
            shuffle=hasattr(original_loader, 'shuffle') and original_loader.shuffle,
            num_workers=num_workers,
            pin_memory=False,  # ç¦ç”¨pin_memoryä»¥é¿å…é—®é¢˜
            persistent_workers=False,  # Windowså…¼å®¹æ€§è®¾ç½®
            multiprocessing_context=None  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        logging.info(f"é‡æ–°åˆ›å»ºDataLoader: num_workers={num_workers}, batch_size={original_loader.batch_size}")
        return new_loader
        
    def _monitor_and_cleanup_memory(self):
        """
        ç›‘æ§å†…å­˜ä½¿ç”¨å¹¶åœ¨å¿…è¦æ—¶è‡ªåŠ¨æ¸…ç†
        """
        if not torch.cuda.is_available():
            return
            
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3    # GB
        
        # å¦‚æœå†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼ï¼Œæ‰§è¡Œè‡ªåŠ¨æ¸…ç†
        if allocated > self.auto_cleanup_threshold:
            logging.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜ ({allocated:.2f}GB > {self.auto_cleanup_threshold}GB)ï¼Œæ‰§è¡Œè‡ªåŠ¨æ¸…ç†...")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            for _ in range(5):
                gc.collect()
            
            # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # æ£€æŸ¥æ¸…ç†æ•ˆæœ
            new_allocated = torch.cuda.memory_allocated() / 1024**3
            freed_memory = allocated - new_allocated
            
            if freed_memory > 0.1:  # é‡Šæ”¾äº†è¶…è¿‡100MB
                logging.info(f"è‡ªåŠ¨æ¸…ç†å®Œæˆï¼Œé‡Šæ”¾äº† {freed_memory:.2f}GB å†…å­˜")
            else:
                logging.warning("è‡ªåŠ¨æ¸…ç†æ•ˆæœæœ‰é™ï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")
        
        # å®šæœŸè®°å½•å†…å­˜çŠ¶æ€
        logging.debug(f"å†…å­˜ç›‘æ§: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB")
        
    def _emergency_memory_cleanup(self):
        """
        ç´§æ€¥å†…å­˜æ¸…ç†ï¼Œç”¨äºå¤„ç†å†…å­˜ä¸è¶³çš„æƒ…å†µ
        """
        logging.warning("æ‰§è¡Œç´§æ€¥å†…å­˜æ¸…ç†...")
        
        # å¤šæ¬¡å¼ºåˆ¶åƒåœ¾å›æ”¶
        for _ in range(10):
            gc.collect()
        
        if torch.cuda.is_available():
            # æ¸…ç©ºæ‰€æœ‰CUDAç¼“å­˜
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # å°è¯•é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜æ± 
            try:
                torch.cuda.memory._dump_snapshot("emergency_cleanup.pickle")
            except:
                pass
            
            # é‡ç½®å†…å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
            
            allocated = torch.cuda.memory_allocated() / 1024**3
            logging.info(f"ç´§æ€¥æ¸…ç†å®Œæˆï¼Œå½“å‰å†…å­˜ä½¿ç”¨: {allocated:.2f}GB")
        
    def distillation_training(self, train_loader, val_loader) -> Dict[str, float]:
        """
        æ‰§è¡ŒçŸ¥è¯†è’¸é¦è®­ç»ƒ
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            è®­ç»ƒç»“æœç»Ÿè®¡
        """
        if not self.distillation_model:
            self.initialize_distillation()
        
        # æ‰“å°é˜¶æ®µæ ‡é¢˜
        teacher_names = ', '.join(self.teacher_models)
        self._print_phase_header(
            "çŸ¥è¯†è’¸é¦è®­ç»ƒé˜¶æ®µ",
            f"æ•™å¸ˆæ¨¡å‹: {teacher_names} | è’¸é¦è½®æ•°: {self.distillation_epochs} | å­¦ä¹ ç‡: {self.distillation_lr} | æ¸©åº¦: {self.temperature}"
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            self.distillation_model.student.parameters(),
            lr=self.distillation_lr,
            weight_decay=1e-4
        )
        
        distillation_start_time = time.time()
        best_val_loss = float('inf')
        training_stats = {'train_losses': [], 'val_losses': []}
        
        # åˆ›å»ºä¸»è¿›åº¦æ¡
        epoch_pbar = tqdm(
            range(self.distillation_epochs),
            desc=f"{Fore.GREEN}ğŸ“ çŸ¥è¯†è’¸é¦{Style.RESET_ALL}",
            ncols=120,
            leave=True,
            position=0,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}"
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.distillation_epochs
        )
        
        for epoch in epoch_pbar:
            epoch_start_time = time.time()
            # è®­ç»ƒé˜¶æ®µ
            self.distillation_model.train()
            train_loss = 0.0
            train_batches = 0
            
            # åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡è¿›åº¦æ¡
            train_pbar = tqdm(
                enumerate(train_loader),
                total=len(train_loader),
                desc=f"{Fore.BLUE}  ğŸ“š è’¸é¦è®­ç»ƒ{Style.RESET_ALL}",
                leave=False,
                position=1,
                ncols=100,
                disable=len(train_loader) < 10  # å¦‚æœbatchæ•°é‡å¤ªå°‘ï¼Œç¦ç”¨å­è¿›åº¦æ¡
            )
            
            try:
                for batch_idx, batch in train_pbar:
                    data, target = batch['image'].to(self.device), batch['label'].to(self.device)
                    
                    # å¤„ç†ç›®æ ‡å¼ é‡ç»´åº¦ï¼šç§»é™¤å¤šä½™çš„é€šé“ç»´åº¦
                    if target.dim() == 5 and target.size(1) == 1:
                        target = target.squeeze(1)  # ä»[B, 1, H, W, D]å˜ä¸º[B, H, W, D]
                    
                    optimizer.zero_grad()
                    
                    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
                    if self.use_amp:
                        with autocast('cuda'):
                            # å‰å‘ä¼ æ’­
                            student_output = self.distillation_model.student(data)
                            
                            # è®¡ç®—è’¸é¦æŸå¤±
                            distillation_loss = self.distillation_model.compute_distillation_loss(
                                data, target
                            )
                        
                        self.scaler.scale(distillation_loss).backward()
                        self.scaler.step(optimizer)
                        self.scaler.update()
                    else:
                        # å‰å‘ä¼ æ’­
                        student_output = self.distillation_model.student(data)
                        
                        # è®¡ç®—è’¸é¦æŸå¤±
                        distillation_loss = self.distillation_model.compute_distillation_loss(
                            data, target
                        )
                        
                        distillation_loss.backward()
                        optimizer.step()
                    
                    train_loss += distillation_loss.item()
                    train_batches += 1
                    
                    # æ›´æ–°è®­ç»ƒè¿›åº¦æ¡
                    train_pbar.set_postfix({
                        'Loss': f'{distillation_loss.item():.4f}',
                        'Avg': f'{train_loss/train_batches:.4f}',
                        'LR': f'{optimizer.param_groups[0]["lr"]:.6f}',
                        'Mem': self._get_memory_info_str().split('|')[0].strip()
                    })
                    
                    # å†…å­˜ç›‘æ§å’Œè‡ªåŠ¨æ¸…ç†
                    if batch_idx % self.memory_monitor_interval == 0:
                        self._monitor_and_cleanup_memory()
            except RuntimeError as e:
                if "DataLoader worker" in str(e):
                    logging.error(f"è’¸é¦è®­ç»ƒDataLoader workeré”™è¯¯: {e}")
                    logging.info("å°è¯•é‡æ–°åˆ›å»ºè®­ç»ƒDataLoader...")
                    train_loader = self._recreate_dataloader(train_loader, num_workers=0)
                    continue
                else:
                    raise e
            
            # éªŒè¯é˜¶æ®µ
            self.distillation_model.eval()
            val_loss = 0.0
            val_batches = 0
            
            # åˆ›å»ºéªŒè¯æ‰¹æ¬¡è¿›åº¦æ¡
            val_pbar = tqdm(
                val_loader,
                desc=f"{Fore.MAGENTA}  ğŸ” è’¸é¦éªŒè¯{Style.RESET_ALL}",
                leave=False,
                position=1,
                ncols=100,
                disable=len(val_loader) < 10  # å¦‚æœbatchæ•°é‡å¤ªå°‘ï¼Œç¦ç”¨å­è¿›åº¦æ¡
            )
            
            with torch.no_grad():
                try:
                    for batch in val_pbar:
                        data, target = batch['image'].to(self.device), batch['label'].to(self.device)
                        
                        # å¤„ç†ç›®æ ‡å¼ é‡ç»´åº¦ï¼šç§»é™¤å¤šä½™çš„é€šé“ç»´åº¦
                        if target.dim() == 5 and target.size(1) == 1:
                            target = target.squeeze(1)  # ä»[B, 1, H, W, D]å˜ä¸º[B, H, W, D]
                        
                        if self.use_amp:
                            with autocast('cuda'):
                                batch_loss = self.distillation_model.compute_distillation_loss(
                                    data, target
                                ).item()
                        else:
                            batch_loss = self.distillation_model.compute_distillation_loss(
                                data, target
                            ).item()
                        
                        val_loss += batch_loss
                        val_batches += 1
                        
                        # æ›´æ–°éªŒè¯è¿›åº¦æ¡
                        val_pbar.set_postfix({
                            'Loss': f'{batch_loss:.4f}',
                            'Avg': f'{val_loss/val_batches:.4f}'
                        })
                except RuntimeError as e:
                    if "DataLoader worker" in str(e):
                        logging.error(f"è’¸é¦éªŒè¯DataLoader workeré”™è¯¯: {e}")
                        logging.info("å°è¯•é‡æ–°åˆ›å»ºéªŒè¯DataLoader...")
                        val_loader = self._recreate_dataloader(val_loader, num_workers=0)
                        continue
                    else:
                        raise e
            
            val_loss /= len(val_loader)
            train_loss /= len(train_loader)
            epoch_time = time.time() - epoch_start_time
            
            training_stats['train_losses'].append(train_loss)
            training_stats['val_losses'].append(val_loss)
            
            # æ›´æ–°ä¸»è¿›åº¦æ¡
            epoch_pbar.set_postfix({
                'Train_Loss': f'{train_loss:.4f}',
                'Val_Loss': f'{val_loss:.4f}',
                'Best': f'{best_val_loss:.4f}',
                'LR': f'{optimizer.param_groups[0]["lr"]:.6f}',
                'Time': f'{epoch_time:.1f}s',
                'ETA': self._estimate_remaining_time(epoch + 1, self.distillation_epochs, distillation_start_time)
            })
            
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                training_stats['best_epoch'] = epoch
                
                # æ˜¾ç¤ºæ–°çš„æœ€ä½³ç»“æœ
                tqdm.write(f"{Fore.GREEN}âœ¨ æ–°çš„æœ€ä½³è’¸é¦æŸå¤±: {val_loss:.4f} (Epoch {epoch+1}){Style.RESET_ALL}")
                
                # ä¿å­˜æœ€ä¼˜å­¦ç”Ÿæ¨¡å‹
                student_save_path = self.save_dir / 'best_student_model.pth'
                torch.save({
                    'model_state_dict': self.student_model.state_dict(),
                    'arch_parameters': self.best_architecture['arch_parameters'],
                    'val_loss': val_loss,
                    'epoch': epoch,
                    'training_config': {
                        'teacher_models': self.teacher_models,
                        'dataset_type': self.dataset_type,
                        'temperature': self.temperature,
                        'alpha': self.alpha
                    }
                }, student_save_path)
                
            scheduler.step()
            
            # æ¯ä¸ªepochç»“æŸåæ¸…ç†å†…å­˜ç¼“å­˜
            self._clear_memory_cache()
        
        # å…³é—­è¿›åº¦æ¡
        epoch_pbar.close()
        
        # è’¸é¦è®­ç»ƒå®Œæˆæ€»ç»“
        distillation_time = time.time() - distillation_start_time
        print(f"\n{Fore.GREEN}ğŸ‰ çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆ!{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“Š è’¸é¦ç»Ÿè®¡:{Style.RESET_ALL}")
        print(f"   â€¢ æ€»è€—æ—¶: {self._get_elapsed_time_str(distillation_start_time)}")
        print(f"   â€¢ æœ€ä½³éªŒè¯æŸå¤±: {Fore.YELLOW}{best_val_loss:.4f}{Style.RESET_ALL}")
        print(f"   â€¢ æœ€ä½³è½®æ¬¡: {training_stats['best_epoch'] + 1}")
        print(f"   â€¢ æ•™å¸ˆæ¨¡å‹: {', '.join(self.teacher_models)}")
        print(f"   â€¢ å†…å­˜ä½¿ç”¨: {self._get_memory_info_str()}")
        print(f"   â€¢ å­¦ç”Ÿæ¨¡å‹ä¿å­˜è‡³: {self.save_dir / 'best_student_model.pth'}")
        
        training_stats['best_val_loss'] = best_val_loss
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        stats_save_path = self.save_dir / 'training_stats.json'
        with open(stats_save_path, 'w') as f:
            json.dump(training_stats, f, indent=2)
        
        return training_stats
        
    def full_training_pipeline(self, train_loader, val_loader) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å®Œæ•´è®­ç»ƒç»“æœ
        """
        # åˆå§‹åŒ–è®­ç»ƒå¼€å§‹æ—¶é—´
        self.training_start_time = time.time()
        
        # æ‰“å°æ€»ä½“æµç¨‹æ ‡é¢˜
        print(f"\n{Fore.CYAN}{'='*100}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}ğŸš€ NAS-è’¸é¦é›†æˆå®Œæ•´è®­ç»ƒæµç¨‹{Style.RESET_ALL}")
        print(f"{Fore.WHITE}æ•™å¸ˆæ¨¡å‹: {', '.join(self.teacher_models)}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}æ•°æ®é›†ç±»å‹: {self.dataset_type}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}NASè½®æ•°: {self.nas_epochs} | è’¸é¦è½®æ•°: {self.distillation_epochs}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}ä¿å­˜ç›®å½•: {self.save_dir}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'='*100}{Style.RESET_ALL}\n")
        
        results = {}
        
        # é˜¶æ®µ1ï¼šæ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒ
        print(f"{Fore.MAGENTA}ğŸ“ é˜¶æ®µ 1/3: æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒ{Style.RESET_ALL}")
        pretrain_results = self.pretrain_teacher_models(train_loader, val_loader)
        results['teacher_pretrain'] = pretrain_results
        
        # é˜¶æ®µé—´éš”
        print(f"\n{Fore.CYAN}{'='*50}{Style.RESET_ALL}")
        print(f"{Fore.MAGENTA}ğŸ“ é˜¶æ®µ 2/3: NASæ¶æ„æœç´¢{Style.RESET_ALL}")
        
        # é˜¶æ®µ2ï¼šNASæ¶æ„æœç´¢
        nas_results = self.search_architecture(train_loader, val_loader)
        results['nas_search'] = nas_results
        
        # é˜¶æ®µé—´éš”
        print(f"\n{Fore.CYAN}{'='*50}{Style.RESET_ALL}")
        print(f"{Fore.MAGENTA}ğŸ“ é˜¶æ®µ 3/3: çŸ¥è¯†è’¸é¦è®­ç»ƒ{Style.RESET_ALL}")
        
        # é˜¶æ®µ3ï¼šçŸ¥è¯†è’¸é¦è®­ç»ƒ
        distillation_results = self.distillation_training(train_loader, val_loader)
        results['distillation'] = distillation_results
        
        # å®Œæ•´æµç¨‹æ€»ç»“
        total_time = time.time() - self.training_start_time
        print(f"\n{Fore.GREEN}{'='*100}{Style.RESET_ALL}")
        print(f"{Fore.GREEN}ğŸ‰ NAS-è’¸é¦é›†æˆå®Œæ•´è®­ç»ƒæµç¨‹å®Œæˆ!{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“Š æ€»ä½“ç»Ÿè®¡:{Style.RESET_ALL}")
        print(f"   â€¢ æ€»è®­ç»ƒæ—¶é—´: {self._get_elapsed_time_str(self.training_start_time)}")
        print(f"   â€¢ æ•™å¸ˆæ¨¡å‹æ•°é‡: {len(pretrain_results['pretrained_models'])}")
        print(f"   â€¢ NASæœ€ä½³éªŒè¯æŸå¤±: {Fore.YELLOW}{nas_results['val_loss']:.4f}{Style.RESET_ALL}")
        print(f"   â€¢ è’¸é¦æœ€ä½³éªŒè¯æŸå¤±: {Fore.YELLOW}{distillation_results['best_val_loss']:.4f}{Style.RESET_ALL}")
        print(f"   â€¢ æœ€ç»ˆå†…å­˜ä½¿ç”¨: {self._get_memory_info_str()}")
        print(f"   â€¢ ç»“æœä¿å­˜ç›®å½•: {self.save_dir}")
        print(f"{Fore.GREEN}{'='*100}{Style.RESET_ALL}\n")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_save_path = self.save_dir / 'full_training_results.json'
        with open(results_save_path, 'w') as f:
            # è½¬æ¢tensorä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_results = self._make_serializable(results)
            json.dump(serializable_results, f, indent=2)
        
        return results
        
    def load_trained_model(self, model_path: str) -> nn.Module:
        """
        åŠ è½½è®­ç»ƒå¥½çš„å­¦ç”Ÿæ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŠ è½½çš„å­¦ç”Ÿæ¨¡å‹
        """
        logging.info(f"åŠ è½½è®­ç»ƒå¥½çš„å­¦ç”Ÿæ¨¡å‹: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
        student = SuperNet(
            in_channels=self.in_channels,
            num_classes=self.num_classes,
            base_channels=8,  # å¤§å¹…å‡å°‘base_channelsä»¥èŠ‚çœå†…å­˜
            num_layers=3,  # å‡å°‘å±‚æ•°
            dataset_type=self.dataset_type
        )
        
        # åŠ è½½æ¶æ„å‚æ•°
        student.arch_parameters.load_state_dict(checkpoint['arch_parameters'])
        
        # åŠ è½½æ¨¡å‹æƒé‡
        student.load_state_dict(checkpoint['model_state_dict'])
        
        student = student.to(self.device)
        student.eval()
        
        logging.info("å­¦ç”Ÿæ¨¡å‹åŠ è½½å®Œæˆ")
        
        return student
        
    def _make_serializable(self, obj: Any) -> Any:
        """
        å°†åŒ…å«tensorçš„å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        """
        if isinstance(obj, torch.Tensor):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        else:
            return obj
    
    def _print_phase_header(self, phase_name: str, details: str = ""):
        """æ‰“å°è®­ç»ƒé˜¶æ®µæ ‡é¢˜"""
        self.current_phase = phase_name
        self.phase_start_time = time.time()
        
        print(f"\n{Fore.CYAN}{'='*80}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}ğŸš€ {phase_name}{Style.RESET_ALL}")
        if details:
            print(f"{Fore.WHITE}{details}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
    
    def _get_memory_info_str(self) -> str:
        """è·å–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²"""
        memory_stats = self.get_memory_stats()
        if torch.cuda.is_available():
            return f"GPU: {memory_stats['allocated_gb']:.1f}GB/{memory_stats['total_gpu_memory_gb']:.1f}GB | åˆ©ç”¨ç‡: {memory_stats['memory_utilization_percent']:.1f}%"
        else:
            # CPUæ¨¡å¼ä¸‹è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
            try:
                import psutil
                memory = psutil.virtual_memory()
                return f"ç³»ç»Ÿå†…å­˜: {memory.used / 1024**3:.1f}GB/{memory.total / 1024**3:.1f}GB | åˆ©ç”¨ç‡: {memory.percent:.1f}%"
            except ImportError:
                return "å†…å­˜ä¿¡æ¯ä¸å¯ç”¨"
    
    def _get_elapsed_time_str(self, start_time: float) -> str:
        """è·å–å·²ç”¨æ—¶é—´å­—ç¬¦ä¸²"""
        elapsed = time.time() - start_time
        hours = int(elapsed // 3600)
        minutes = int((elapsed % 3600) // 60)
        seconds = int(elapsed % 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
    
    def _estimate_remaining_time(self, current_step: int, total_steps: int, start_time: float) -> str:
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if current_step == 0:
            return "--:--:--"
        
        elapsed = time.time() - start_time
        avg_time_per_step = elapsed / current_step
        remaining_steps = total_steps - current_step
        remaining_time = remaining_steps * avg_time_per_step
        
        hours = int(remaining_time // 3600)
        minutes = int((remaining_time % 3600) // 60)
        seconds = int(remaining_time % 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def get_memory_stats(self) -> Dict[str, float]:
        """
        è·å–è¯¦ç»†çš„å†…å­˜ä½¿ç”¨ç»Ÿè®¡
        
        Returns:
            å†…å­˜ç»Ÿè®¡å­—å…¸
        """
        stats = {}
        
        if torch.cuda.is_available():
            stats.update({
                'allocated_gb': torch.cuda.memory_allocated() / 1024**3,
                'reserved_gb': torch.cuda.memory_reserved() / 1024**3,
                'max_allocated_gb': torch.cuda.max_memory_allocated() / 1024**3,
                'max_reserved_gb': torch.cuda.max_memory_reserved() / 1024**3,
                'total_gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3,
                'memory_utilization_percent': (torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory) * 100
            })
        else:
            stats = {
                'allocated_gb': 0.0,
                'reserved_gb': 0.0,
                'max_allocated_gb': 0.0,
                'max_reserved_gb': 0.0,
                'total_gpu_memory_gb': 0.0,
                'memory_utilization_percent': 0.0
            }
            
        return stats
            
    def get_save_paths(self) -> Dict[str, str]:
        """
        è·å–æ‰€æœ‰ä¿å­˜è·¯å¾„
        
        Returns:
            ä¿å­˜è·¯å¾„å­—å…¸
        """
        return {
            'best_architecture': str(self.save_dir / 'best_architecture.pth'),
            'best_student_model': str(self.save_dir / 'best_student_model.pth'),
            'training_stats': str(self.save_dir / 'training_stats.json'),
            'full_results': str(self.save_dir / 'full_training_results.json'),
            'save_directory': str(self.save_dir)
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import torch
    from torch.utils.data import DataLoader, TensorDataset
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¨¡æ‹ŸBraTSæ•°æ®
    batch_size = 2
    data = torch.randn(batch_size * 10, 4, 32, 32, 32)
    targets = torch.randint(0, 4, (batch_size * 10, 16, 16, 16))  # åŒ¹é…æ¨¡å‹è¾“å‡ºå°ºå¯¸
    
    dataset = TensorDataset(data, targets)
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºNAS-è’¸é¦é›†æˆå™¨
    teacher_models = ['UNet3D', 'VNet3D', 'ResUNet3D']
    
    nas_distillation = NASDistillationIntegration(
        teacher_models=teacher_models,
        device=device,
        dataset_type='BraTS',
        nas_epochs=5,  # æµ‹è¯•ç”¨è¾ƒå°‘è½®æ•°
        distillation_epochs=5,
        save_dir='./test_checkpoints/nas_distillation'
    )
    
    try:
        # æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
        results = nas_distillation.full_training_pipeline(train_loader, val_loader)
        print("NAS-è’¸é¦é›†æˆæµ‹è¯•æˆåŠŸï¼")
        print(f"ä¿å­˜è·¯å¾„: {nas_distillation.get_save_paths()}")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()