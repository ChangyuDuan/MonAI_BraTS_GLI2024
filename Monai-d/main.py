import os
import sys
import argparse
import torch
from pathlib import Path
from typing import Dict, Any
import warnings

# è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥ä»¥é¿å…å†…å­˜ç¢ç‰‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# è¿‡æ»¤PyTorch TorchScriptç›¸å…³çš„å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", message=".*TorchScript.*functional optimizers.*deprecated.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="torch.distributed.optim")

from DatasetLoader_transforms import DatasetLoader
from MSMultiSpineLoader import MSMultiSpineDatasetLoader
from model import BasicModelBank, get_all_supported_models
from train import ModelTrainer
from evaluate import ModelEvaluator
from inference import InferenceEngine

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ä¸­æ–‡å­—ä½“é…ç½®
try:
    from font_config import configure_chinese_font
    # è‡ªåŠ¨é…ç½®ä¸­æ–‡å­—ä½“
    configure_chinese_font()
except ImportError:
    import warnings
    warnings.warn("æœªæ‰¾åˆ°font_configæ¨¡å—ï¼Œä¸­æ–‡æ˜¾ç¤ºå¯èƒ½å‡ºç°é—®é¢˜", UserWarning)

def get_high_performance_config(device='auto') -> Dict[str, Any]:
    """
    è·å–é…ç½®æ€§èƒ½ï¼ˆé’ˆå¯¹CPUå’ŒGPUè®¾å¤‡ï¼‰
    é»˜è®¤ç»Ÿä¸€ä½¿ç”¨è‡ªé€‚åº”æŸå¤±å‡½æ•°ç­–ç•¥å’Œå®Œæ•´è¯„ä¼°æŒ‡æ ‡
    """
    # è®¾å¤‡æ€§èƒ½é…ç½®
    if device == 'cpu':
        batch_size = 4  
        cache_rate = 0.5  
        num_workers = 0  # Windowsä¸Šè®¾ç½®ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
        pin_memory = False
        spatial_size = (96, 96, 96)
        roi_size = (64, 64, 64)
        use_amp = False
    else:  # cuda or auto
        # GPUæ€§èƒ½é…ç½®
         batch_size = 8
         cache_rate = 1.0
         num_workers = 16
         pin_memory = True
         spatial_size = (160, 160, 160)
         roi_size = (128, 128, 128)
         use_amp = True

    
    # åŸºç¡€å­¦ä¹ ç‡
    learning_rate = 2e-4
    
    return {
        # æ•°æ®åŠ è½½é…ç½®
        'batch_size': batch_size,
        'cache_rate': cache_rate,
        'num_workers': num_workers,
        'pin_memory': pin_memory,
        'persistent_workers': True,
        'prefetch_factor': 4,
        
        # æ•°æ®é¢„å¤„ç†é…ç½®
        'spatial_size': spatial_size,
        'roi_size': roi_size,
        
        # è®­ç»ƒé…ç½®
        'max_epochs': 500,
        'learning_rate': learning_rate,
        'weight_decay': 1e-5,
        'use_amp': use_amp,
        'patience': 50,
        'save_interval': 10,
        'log_interval': 5,
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        'optimizer_name': 'AdamW',
        'scheduler_name': 'CosineAnnealingLR',
        
        # éªŒè¯é…ç½®
        'val_interval': 1,
        'val_split': 0.2,
        
        # ç»Ÿä¸€ç­–ç•¥é…ç½®ï¼ˆæ‰€æœ‰æ¨¡å‹ç±»å‹éƒ½ä½¿ç”¨ï¼‰
        'use_adaptive_loss': True,          # å¼ºåˆ¶ä½¿ç”¨è‡ªé€‚åº”æŸå¤±å‡½æ•°
        'use_full_metrics': True,           # å¼ºåˆ¶ä½¿ç”¨å®Œæ•´è¯„ä¼°æŒ‡æ ‡
        'loss_strategy': 'adaptive_combined', # æŸå¤±å‡½æ•°ç­–ç•¥
        'metrics_strategy': 'comprehensive', # è¯„ä¼°æŒ‡æ ‡ç­–ç•¥
        
        # å…¶ä»–é…ç½®
        'seed': 42,
        'gradient_clip_val': 1.0,
        'compile_model': True,  # æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
        'deterministic': True
    }

def detect_dataset_type(data_dir: str) -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹
    """
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºMS_MultiSpineæ•°æ®é›†
        if os.path.exists(os.path.join(data_dir, 'MS_MultiSpine_dataset')):
            return 'MS_MultiSpine'
        
        # æ£€æŸ¥ç›®å½•ä¸­çš„æ–‡ä»¶ç»“æ„
        subdirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]
        
        # MS_MultiSpineæ•°æ®é›†ç‰¹å¾ï¼šåŒ…å«sub-xxxæ ¼å¼çš„ç›®å½•
        ms_pattern_count = sum(1 for d in subdirs if d.startswith('sub-') and d[4:].isdigit())
        if ms_pattern_count > 0:
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åŒ…å«MSç‰¹æœ‰çš„æ–‡ä»¶
            for subdir in subdirs[:3]:  # æ£€æŸ¥å‰3ä¸ªç›®å½•
                subdir_path = os.path.join(data_dir, subdir)
                files = os.listdir(subdir_path)
                # æ£€æŸ¥æ˜¯å¦åŒ…å«T2å’Œå…¶ä»–æ¨¡æ€æ–‡ä»¶
                has_t2 = any('T2' in f for f in files)
                has_lesion_mask = any('LESIONMASK' in f for f in files)
                if has_t2 and has_lesion_mask:
                    return 'MS_MultiSpine'
        
        # BraTSæ•°æ®é›†ç‰¹å¾ï¼šåŒ…å«BraTSæ ¼å¼çš„ç›®å½•æˆ–æ–‡ä»¶
        brats_pattern_count = sum(1 for d in subdirs if 'BraTS' in d or 'brats' in d.lower())
        if brats_pattern_count > 0:
            return 'BraTS'
        
        # é»˜è®¤è¿”å›BraTS
        return 'BraTS'
        
    except Exception as e:
        print(f"æ•°æ®é›†ç±»å‹æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤BraTSç±»å‹")
        return 'BraTS'

def merge_args_with_config(args, device) -> Dict[str, Any]:
    # è·å–åŸºç¡€é…ç½®
    config = get_high_performance_config(
        device=str(device)
    )
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é»˜è®¤é…ç½®
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.epochs:
        config['max_epochs'] = args.epochs
    if args.learning_rate:
        config['learning_rate'] = args.learning_rate
        
    # æ•°æ®é›†ç±»å‹æ£€æµ‹å’Œé…ç½®
    dataset_type = getattr(args, 'dataset_type', None)
    if dataset_type is None or dataset_type == 'auto':
        # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹
        detected_type = detect_dataset_type(args.data_dir)
        print(f"è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹: {detected_type}")
        dataset_type = detected_type
    else:
        print(f"ä½¿ç”¨æŒ‡å®šæ•°æ®é›†ç±»å‹: {dataset_type}")
        
    # æ·»åŠ åŸºç¡€å‚æ•°
    config['data_dir'] = args.data_dir
    config['output_dir'] = args.output_dir
    config['device'] = str(device)
    config['dataset_type'] = dataset_type
    
    # æ·»åŠ å¤åˆæ¶æ„æ¨¡å‹é…ç½®
    config['model_category'] = getattr(args, 'model_category', 'basic')
    config['model_type'] = getattr(args, 'model_type', 'fusion')
    
    # åˆ›å»ºmodel_kwargså­—å…¸æ¥é›†ä¸­å¤„ç†æ‰€æœ‰æ¨¡å‹ç›¸å…³å‚æ•°
    model_kwargs = {}
    
    # NASç›¸å…³å‚æ•°
    if hasattr(args, 'nas_type'):
        model_kwargs['nas_type'] = args.nas_type
    if hasattr(args, 'base_channels'):
        model_kwargs['base_channels'] = args.base_channels
    if hasattr(args, 'num_layers'):
        model_kwargs['num_layers'] = args.num_layers
    if hasattr(args, 'arch_lr'):
        model_kwargs['arch_lr'] = args.arch_lr
    if hasattr(args, 'model_lr'):
        model_kwargs['model_lr'] = args.model_lr
    if hasattr(args, 'max_layers'):
        model_kwargs['max_layers'] = args.max_layers
    if hasattr(args, 'start_layers'):
        model_kwargs['start_layers'] = args.start_layers
    if hasattr(args, 'auto_adjust'):
        model_kwargs['auto_adjust'] = args.auto_adjust
    if hasattr(args, 'nas_epochs'):
        model_kwargs['nas_epochs'] = args.nas_epochs
    if hasattr(args, 'distillation_epochs'):
        model_kwargs['distillation_epochs'] = args.distillation_epochs
    if hasattr(args, 'distillation_lr'):
        model_kwargs['distillation_lr'] = args.distillation_lr
    
    # çŸ¥è¯†è’¸é¦å‚æ•°ï¼ˆæ”¾å…¥model_kwargsï¼‰
    if hasattr(args, 'distillation_temperature'):
        model_kwargs['distillation_temperature'] = args.distillation_temperature
    if hasattr(args, 'distillation_alpha'):
        model_kwargs['distillation_alpha'] = args.distillation_alpha
    
    # èåˆç½‘ç»œå‚æ•°
    if hasattr(args, 'fusion_type'):
        model_kwargs['fusion_type'] = args.fusion_type
    if hasattr(args, 'fusion_channels'):
        model_kwargs['fusion_channels'] = args.fusion_channels
    
    # å°†model_kwargsæ·»åŠ åˆ°configä¸­
    config['model_kwargs'] = model_kwargs
    
    # çŸ¥è¯†è’¸é¦é…ç½®ï¼ˆé¡¶å±‚é…ç½®ï¼‰
    if hasattr(args, 'teacher_models') and args.teacher_models:
        config['teacher_models'] = args.teacher_models
    elif config['model_category'] == 'advanced' and config['model_type'] == 'distillation':
        # é»˜è®¤ä½¿ç”¨æ‰€æœ‰ç½‘ç»œæ¶æ„ä½œä¸ºæ•™å¸ˆæ¨¡å‹
        config['teacher_models'] = get_all_supported_models()
    if hasattr(args, 'student_model') and args.student_model:
        config['student_model'] = args.student_model
    
    # çŸ¥è¯†è’¸é¦å‚æ•°éªŒè¯ï¼šç¡®ä¿æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¸é‡å¤
    if config['model_category'] == 'advanced' and config['model_type'] == 'distillation':
        student_model = config.get('student_model', 'VNet3D')
        teacher_models = config.get('teacher_models', [])
        
        # æ£€æŸ¥æ•™å¸ˆæ¨¡å‹åˆ—è¡¨ä¸­æ˜¯å¦åŒ…å«å­¦ç”Ÿæ¨¡å‹
        if isinstance(teacher_models, list) and student_model in teacher_models:
            print(f"âš  è­¦å‘Šï¼šæ£€æµ‹åˆ°æ•™å¸ˆæ¨¡å‹åˆ—è¡¨ä¸­åŒ…å«å­¦ç”Ÿæ¨¡å‹ '{student_model}'")
            # è‡ªåŠ¨ä»æ•™å¸ˆæ¨¡å‹åˆ—è¡¨ä¸­ç§»é™¤å­¦ç”Ÿæ¨¡å‹
            config['teacher_models'] = [name for name in teacher_models if name != student_model]
            print(f"âœ“ å·²è‡ªåŠ¨ä»æ•™å¸ˆæ¨¡å‹åˆ—è¡¨ä¸­ç§»é™¤å­¦ç”Ÿæ¨¡å‹ï¼Œå½“å‰æ•™å¸ˆæ¨¡å‹: {config['teacher_models']}")
            
            # å¦‚æœç§»é™¤åæ•™å¸ˆæ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é™¤å­¦ç”Ÿæ¨¡å‹å¤–çš„æ‰€æœ‰æ¨¡å‹
            if not config['teacher_models']:
                all_models = get_all_supported_models()
                config['teacher_models'] = [name for name in all_models if name != student_model]
                print(f"âœ“ æ•™å¸ˆæ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œè‡ªåŠ¨ä½¿ç”¨é™¤å­¦ç”Ÿæ¨¡å‹å¤–çš„æ‰€æœ‰æ¨¡å‹: {config['teacher_models']}")
        
        # éªŒè¯æœ€ç»ˆé…ç½®
        if not config.get('teacher_models'):
            raise ValueError("çŸ¥è¯†è’¸é¦éœ€è¦è‡³å°‘ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹")
        
        print(f"ğŸ“š çŸ¥è¯†è’¸é¦å‚æ•°éªŒè¯é€šè¿‡ï¼š")
        print(f"  å­¦ç”Ÿæ¨¡å‹: {student_model}")
        print(f"  æ•™å¸ˆæ¨¡å‹: {config['teacher_models']}")
        print(f"  æ•™å¸ˆæ¨¡å‹æ•°é‡: {len(config['teacher_models'])}")
    
    # èåˆç½‘ç»œé…ç½®
    if hasattr(args, 'fusion_models') and args.fusion_models:
        config['fusion_models'] = args.fusion_models
    elif config['model_category'] == 'advanced' and config['model_type'] == 'fusion':
        # é»˜è®¤ä½¿ç”¨æ‰€æœ‰7ä¸ªç½‘ç»œæ¶æ„è¿›è¡Œèåˆ
        config['fusion_models'] = get_all_supported_models()
    
    # NASé…ç½®
    if hasattr(args, 'nas_epochs'):
        config['nas_epochs'] = args.nas_epochs
    
    # NAS-è’¸é¦é›†æˆé…ç½®
    if hasattr(args, 'distillation_epochs'):
        config['distillation_epochs'] = args.distillation_epochs
    if hasattr(args, 'distillation_lr'):
        config['distillation_lr'] = args.distillation_lr
    if hasattr(args, 'nas_distillation_save_dir'):
        config['save_dir'] = args.nas_distillation_save_dir
    
    # é¢„è®­ç»ƒé…ç½®
    if hasattr(args, 'pretrained_dir'):
        config['pretrained_dir'] = args.pretrained_dir
    # å¤„ç†é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹å‚æ•°
    config['pretrain_teachers'] = getattr(args, 'pretrain_teachers', True)
    if hasattr(args, 'teacher_epochs'):
        config['teacher_epochs'] = args.teacher_epochs
    if hasattr(args, 'force_retrain_teachers'):
        config['force_retrain_teachers'] = args.force_retrain_teachers
    
    # æ¨ç†ç›¸å…³å‚æ•°
    if hasattr(args, 'roi_size'):
        config['roi_size'] = args.roi_size
    if hasattr(args, 'sw_batch_size'):
        config['sw_batch_size'] = args.sw_batch_size
    if hasattr(args, 'overlap'):
        config['overlap'] = args.overlap
    if hasattr(args, 'batch_inference'):
        config['batch_inference'] = args.batch_inference
    if hasattr(args, 'no_visualization'):
        config['no_visualization'] = args.no_visualization
    
    return config

def run_simplified_training(args, device: torch.device):
    """
    è¿è¡Œè®­ç»ƒæµç¨‹
    """
    print("\n" + "=" * 60)
    
    print("å¼€å§‹è®­ç»ƒæµç¨‹")
    print("=" * 60)
    
    try:
        # è‡ªåŠ¨è°ƒèŠ‚å‚æ•°
        if args.auto_adjust:
            batch_size, epochs, learning_rate = auto_adjust_parameters(device, args)
        else:
            batch_size = args.batch_size or 2
            epochs = args.epochs or 500
            learning_rate = args.learning_rate or 1e-4
        
        print(f"è®­ç»ƒå‚æ•°:")
        print(f"  æ•°æ®ç›®å½•: {args.data_dir}")
        print(f"  æ¨¡å‹åˆ—è¡¨: {args.model_names}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  è®­ç»ƒè½®æ•°: {epochs}")
        print(f"  å­¦ä¹ ç‡: {learning_rate}")
        print(f"  è®¡ç®—è®¾å¤‡: {device}")

        print(f"  è‡ªåŠ¨è°ƒèŠ‚: {args.auto_adjust}")
        print("-" * 60)
        
        trained_models = []
        
        # é¦–å…ˆåˆ¤æ–­è®­ç»ƒæ¨¡å¼å¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
        if args.model_category == 'basic':
            # åŸºç¡€æ¨¡å‹è®­ç»ƒæ¨¡å¼åˆ¤æ–­
            if len(args.model_names) > 1:
                # å¤šæ¨¡å‹è®­ç»ƒï¼ˆåŸºç¡€æ¨¡å‹ï¼‰
                if args.parallel:
                    print(f"\nå¯åŠ¨å¤šæ¨¡å‹å¹¶è¡Œè®­ç»ƒæ¨¡å¼ - åŒæ—¶è®­ç»ƒ{len(args.model_names)}ä¸ªåŸºç¡€æ¨¡å‹")
                else:
                    print(f"\nå¯åŠ¨å¤šæ¨¡å‹é€ä¸ªè®­ç»ƒæ¨¡å¼ - ä¾æ¬¡è®­ç»ƒ{len(args.model_names)}ä¸ªåŸºç¡€æ¨¡å‹")
            else:
                # å•æ¨¡å‹è®­ç»ƒï¼ˆåŸºç¡€æ¨¡å‹ï¼‰
                print(f"\nå¯åŠ¨å•æ¨¡å‹è®­ç»ƒ: {args.model_names[0].upper()}")
        elif args.model_category == 'advanced':
            # å¤åˆæ¶æ„æ¨¡å‹è®­ç»ƒæ¨¡å¼åˆ¤æ–­å’Œè®­ç»ƒé€»è¾‘
            model_type_name = {
                'fusion': 'èåˆç½‘ç»œ',
                'distillation': 'çŸ¥è¯†è’¸é¦', 
                'nas': 'ç¥ç»æ¶æ„æœç´¢',
                'nas_distillation': 'NAS-è’¸é¦é›†æˆ'
            }.get(args.model_type, args.model_type)
            print(f"\nå¯åŠ¨å¤åˆæ¶æ„æ¨¡å‹è®­ç»ƒ: {model_type_name.upper()}")
            
            # å¤åˆæ¶æ„æ¨¡å‹è®­ç»ƒ
            if args.model_type == 'distillation':
                print(f"å­¦ç”Ÿæ¨¡å‹: {args.student_model}")
                if hasattr(args, 'teacher_models') and args.teacher_models:
                    print(f"æ•™å¸ˆæ¨¡å‹: {', '.join(args.teacher_models)}")
            elif args.model_type == 'nas_distillation':
                print(f"NAS-è’¸é¦é›†æˆæ¨¡å¼ï¼š")
                print(f"  NASæœç´¢è½®æ•°: {getattr(args, 'nas_epochs', 50)}")
                print(f"  çŸ¥è¯†è’¸é¦è½®æ•°: {getattr(args, 'distillation_epochs', 100)}")
                if hasattr(args, 'teacher_models') and args.teacher_models:
                    print(f"  æ•™å¸ˆæ¨¡å‹: {', '.join(args.teacher_models)}")
                print(f"  ä¿å­˜ç›®å½•: {getattr(args, 'nas_distillation_save_dir', './checkpoints/nas_distillation')}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
                pretrain_enabled = getattr(args, 'pretrain_teachers', True)
                if pretrain_enabled:
                    print(f"\nå¼€å§‹é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
                    
                    # è·å–éœ€è¦é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹åˆ—è¡¨
                    teacher_models = getattr(args, 'teacher_models', get_all_supported_models())
                    pretrained_dir = getattr(args, 'pretrained_dir', './pretrained_teachers')
                    teacher_epochs = getattr(args, 'teacher_epochs', 100)
                    force_retrain = getattr(args, 'force_retrain_teachers', False)
                    
                    pretrained_paths = {}
                    
                    # ä½¿ç”¨ç°æœ‰çš„åŸºç¡€æ¨¡å‹è®­ç»ƒé€»è¾‘é¢„è®­ç»ƒæ¯ä¸ªæ•™å¸ˆæ¨¡å‹
                    for teacher_model in teacher_models:
                        teacher_output_dir = f"{pretrained_dir}/{teacher_model}"
                        teacher_model_path = f"{teacher_output_dir}/best_model.pth"
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹
                        if os.path.exists(teacher_model_path) and not force_retrain:
                            print(f"æ•™å¸ˆæ¨¡å‹ {teacher_model} å·²å­˜åœ¨é¢„è®­ç»ƒæƒé‡ï¼Œè·³è¿‡è®­ç»ƒ")
                            pretrained_paths[teacher_model] = teacher_model_path
                            continue
                        
                        print(f"\né¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹: {teacher_model.upper()}")
                        
                        # åˆ›å»ºæ•™å¸ˆæ¨¡å‹è®­ç»ƒé…ç½®
                        teacher_config = merge_args_with_config(args, device)
                        teacher_config.update({
                            'batch_size': batch_size,
                            'max_epochs': teacher_epochs,
                            'learning_rate': learning_rate,
                            'model_name': teacher_model,
                            'output_dir': teacher_output_dir
                        })
                        
                        # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒæ•™å¸ˆæ¨¡å‹
                        teacher_trainer = ModelTrainer(teacher_config)
                        teacher_trainer.train()
                        
                        # æ£€æŸ¥è®­ç»ƒç»“æœ
                        if os.path.exists(teacher_model_path):
                            pretrained_paths[teacher_model] = teacher_model_path
                            print(f"[æˆåŠŸ] æ•™å¸ˆæ¨¡å‹ {teacher_model.upper()} é¢„è®­ç»ƒå®Œæˆ")
                        else:
                            print(f"[å¤±è´¥] æ•™å¸ˆæ¨¡å‹ {teacher_model.upper()} é¢„è®­ç»ƒå¤±è´¥")
                    
                    print(f"\næ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒå®Œæˆ: {len(pretrained_paths)} ä¸ªæ¨¡å‹")
                    for model_name, path in pretrained_paths.items():
                        print(f"  {model_name}: {path}")
        
        # æ ¹æ®æ¨¡å‹ç±»åˆ«é€‰æ‹©è®­ç»ƒæ–¹å¼
        if args.model_category == 'advanced':
            # å¤åˆæ¶æ„æ¨¡å‹è®­ç»ƒ
            model_type_name = {
                'fusion': 'èåˆç½‘ç»œ',
                'distillation': 'çŸ¥è¯†è’¸é¦', 
                'nas': 'ç¥ç»æ¶æ„æœç´¢'
            }.get(args.model_type, args.model_type)
            print(f"\nå¼€å§‹è®­ç»ƒå¤åˆæ¶æ„æ¨¡å‹: {model_type_name.upper()}")
            
            # åˆ›å»ºè®­ç»ƒé…ç½®
            config = merge_args_with_config(args, device)
            output_subdir = f"{args.model_type}_model"
            config.update({
                'batch_size': batch_size,
                'max_epochs': epochs,
                'learning_rate': learning_rate,
                'output_dir': f'{args.output_dir}/models/{output_subdir}'
            })
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = ModelTrainer(config)
            trainer.train()
            
            # æ£€æŸ¥è®­ç»ƒç»“æœ
            model_path = f'{args.output_dir}/models/{output_subdir}/checkpoints/best_model.pth'
            if os.path.exists(model_path):
                trained_models.append({
                    'name': args.model_type,
                    'path': model_path,
                    'config': config
                })
                print(f"[æˆåŠŸ] {model_type_name.upper()} è®­ç»ƒå®Œæˆ")
            else:
                print(f"[å¤±è´¥] {model_type_name.upper()} è®­ç»ƒå¤±è´¥")
        
        elif args.model_category == 'basic':
            # åŸºç¡€æ¨¡å‹è®­ç»ƒ
            if args.parallel:
                # å¹¶è¡Œè®­ç»ƒæ‰€æœ‰åŸºç¡€æ¨¡å‹
                import threading
                import queue
                
                print(f"\nå¯åŠ¨åŸºç¡€æ¨¡å‹å¹¶è¡Œè®­ç»ƒ - åŒæ—¶è®­ç»ƒ{len(args.model_names)}ä¸ªåŸºç¡€æ¨¡å‹")
                
                # åˆ›å»ºç»“æœé˜Ÿåˆ—å’Œçº¿ç¨‹åˆ—è¡¨
                result_queue = queue.Queue()
                threads = []
                
                def train_model_thread(model_name, config, result_queue):
                    """å•ä¸ªæ¨¡å‹è®­ç»ƒçš„çº¿ç¨‹å‡½æ•°"""
                    try:
                        print(f"[çº¿ç¨‹] å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name.upper()}")
                        trainer = ModelTrainer(config)
                        trainer.train()
                        
                        # æ£€æŸ¥è®­ç»ƒç»“æœ
                        model_path = f'{config["output_dir"]}/checkpoints/best_model.pth'
                        if os.path.exists(model_path):
                            result_queue.put({
                                'success': True,
                                'name': model_name,
                                'path': model_path,
                                'config': config
                            })
                            print(f"[çº¿ç¨‹] {model_name.upper()} è®­ç»ƒå®Œæˆ")
                        else:
                            result_queue.put({
                                'success': False,
                                'name': model_name,
                                'error': 'æ¨¡å‹æ–‡ä»¶æœªç”Ÿæˆ'
                            })
                            print(f"[çº¿ç¨‹] {model_name.upper()} è®­ç»ƒå¤±è´¥")
                    except Exception as e:
                        result_queue.put({
                            'success': False,
                            'name': model_name,
                            'error': str(e)
                        })
                        print(f"[çº¿ç¨‹] {model_name.upper()} è®­ç»ƒå‡ºé”™: {str(e)}")
                
                # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºè®­ç»ƒçº¿ç¨‹
                for model_name in args.model_names:
                    config = merge_args_with_config(args, device)
                    config.update({
                        'batch_size': batch_size,
                        'max_epochs': epochs,
                        'learning_rate': learning_rate,
                        'model_name': model_name,
                        'output_dir': f'{args.output_dir}/models/{model_name}'
                    })
                    
                    thread = threading.Thread(
                        target=train_model_thread,
                        args=(model_name, config, result_queue),
                        name=f"Train-{model_name}"
                    )
                    threads.append(thread)
                    thread.start()
                    print(f"[ä¸»çº¿ç¨‹] å¯åŠ¨ {model_name.upper()} è®­ç»ƒçº¿ç¨‹")
                
                # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
                print(f"\nç­‰å¾…æ‰€æœ‰{len(threads)}ä¸ªè®­ç»ƒçº¿ç¨‹å®Œæˆ...")
                for thread in threads:
                    thread.join()
                
                # æ”¶é›†æ‰€æœ‰è®­ç»ƒç»“æœ
                print(f"\næ”¶é›†è®­ç»ƒç»“æœ...")
                while not result_queue.empty():
                    result = result_queue.get()
                    if result['success']:
                        trained_models.append({
                            'name': result['name'],
                            'path': result['path'],
                            'config': result['config']
                        })
                        print(f"[æˆåŠŸ] {result['name'].upper()} å¹¶è¡Œè®­ç»ƒå®Œæˆ")
                    else:
                        print(f"[å¤±è´¥] {result['name'].upper()} å¹¶è¡Œè®­ç»ƒå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        else:
            # é€ä¸ªè®­ç»ƒæ‰€æœ‰åŸºç¡€æ¨¡å‹
            print(f"\nå¯åŠ¨åŸºç¡€æ¨¡å‹é€ä¸ªè®­ç»ƒ - ä¾æ¬¡è®­ç»ƒ{len(args.model_names)}ä¸ªåŸºç¡€æ¨¡å‹")
            
            # é€ä¸ªè®­ç»ƒæ¯ä¸ªæ¨¡å‹
            for i, model_name in enumerate(args.model_names, 1):
                print(f"\n[{i}/{len(args.model_names)}] å¼€å§‹è®­ç»ƒåŸºç¡€æ¨¡å‹: {model_name.upper()}")
                
                # åˆ›å»ºè®­ç»ƒé…ç½®
                config = merge_args_with_config(args, device)
                config.update({
                    'model_name': model_name,
                    'batch_size': batch_size,
                    'max_epochs': epochs,
                    'learning_rate': learning_rate,
                    'output_dir': f'{args.output_dir}/models/{model_name}'
                })
                
                # åˆ›å»ºè®­ç»ƒå™¨
                trainer = ModelTrainer(config)
                trainer.train()
                
                # æ£€æŸ¥è®­ç»ƒç»“æœ
                model_path = f'{args.output_dir}/models/{model_name}/checkpoints/best_model.pth'
                if os.path.exists(model_path):
                    trained_models.append({
                        'name': model_name,
                        'path': model_path,
                        'config': config
                    })
                    print(f"[æˆåŠŸ] {model_name.upper()} è®­ç»ƒå®Œæˆ")
                else:
                    print(f"[å¤±è´¥] {model_name.upper()} è®­ç»ƒå¤±è´¥")
            
            print(f"\nåŸºç¡€æ¨¡å‹é€ä¸ªè®­ç»ƒå®Œæˆï¼Œå…±è®­ç»ƒäº† {len(trained_models)} ä¸ªæ¨¡å‹")
        
        # è®­ç»ƒæµç¨‹å®Œæˆ 
        print(f"\n[æˆåŠŸ] è®­ç»ƒæµç¨‹å®Œæˆï¼")
        print(f"è®­ç»ƒäº† {len(trained_models)} ä¸ªæ¨¡å‹")
        
        return True
        
    except Exception as e:
        print(f"\n[é”™è¯¯] è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def auto_adjust_parameters(device: torch.device, args):
    """
    æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒèŠ‚è®­ç»ƒå‚æ•°
    """
    # åŸºç¡€å‚æ•°
    base_batch_size = args.batch_size or 2
    base_epochs = args.epochs or 500
    base_lr = args.learning_rate or 1e-4
    
    if device.type == 'cuda':
        # GPUè®¾å¤‡ï¼Œä½¿ç”¨ä¼˜åŒ–å‚æ•°
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        batch_size = base_batch_size * 2  # GPUå¯ä»¥å¤„ç†æ›´å¤§æ‰¹æ¬¡
        epochs = base_epochs
        learning_rate = base_lr * 1.2  # ç¨å¾®æé«˜å­¦ä¹ ç‡åŠ é€Ÿè®­ç»ƒ
        
        print(f"GPUä¼˜åŒ–é…ç½® (æ˜¾å­˜: {gpu_memory_gb:.1f}GB)")
    else:
        # CPUè®¾å¤‡ï¼Œä½¿ç”¨åˆç†å‚æ•°
        batch_size = base_batch_size  # CPUå¯ä»¥å¤„ç†æ­£å¸¸æ‰¹æ¬¡
        epochs = base_epochs  # ä¿æŒæ­£å¸¸è®­ç»ƒè½®æ•°
        learning_rate = base_lr  # ä½¿ç”¨æ ‡å‡†å­¦ä¹ ç‡
        
        print(f"CPUä¼˜åŒ–é…ç½®")
    
    return batch_size, epochs, learning_rate


def run_evaluation(model_path: str = None, data_dir: str = None, output_dir: str = './evaluation_results', device: torch.device = None):
    """
    è¿è¡Œè¯„ä¼°æµç¨‹
    model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    data_dir: æ•°æ®ç›®å½•
    output_dir: è¾“å‡ºç›®å½•
    device: è®¡ç®—è®¾å¤‡
    """
    print("\n" + "=" * 80)
    print("åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹è¯„ä¼°")
    print("=" * 80)
    
    try:
        # å¦‚æœæœªæŒ‡å®šè®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼Œå°†æ ¹æ®æ—¶é—´ä¼˜å…ˆåŸåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        if model_path is None:
            outputs_dir = './outputs'
            if os.path.exists(outputs_dir):
                # æŸ¥æ‰¾outputsç›®å½•ä¸‹çš„æ‰€æœ‰æ¨¡å‹ç›®å½•
                model_dirs = [d for d in os.listdir(outputs_dir) 
                             if os.path.isdir(os.path.join(outputs_dir, d))]
                if model_dirs:
                    # é€‰æ‹©æœ€æ–°çš„æ¨¡å‹ç›®å½•
                    latest_model_dir = max(model_dirs, 
                                         key=lambda x: os.path.getmtime(os.path.join(outputs_dir, x)))
                    model_path = os.path.join(outputs_dir, latest_model_dir)
                    print(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°è®­ç»ƒçš„æ¨¡å‹: {model_path}")
                else:
                    print(f"[é”™è¯¯] åœ¨ {outputs_dir} ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒå¥½çš„æ¨¡å‹")
                    return False
            else:
                print(f"[é”™è¯¯] è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {outputs_dir}")
                return False
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(model_path):
            print(f"[é”™è¯¯] æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
        
        # æ˜¾ç¤ºè¯„ä¼°é…ç½®ä¿¡æ¯
        print(f"\n è¯„ä¼°é…ç½®ä¿¡æ¯:")
        print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"  æ•°æ®ç›®å½•: {data_dir}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        if device is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"\n è®¡ç®—è®¾å¤‡: {device}")
        if device.type == 'cuda':
            print(f"  GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
            print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        print("\n" + "-" * 80)
        print("åˆå§‹åŒ–è¯„ä¼°å™¨...")
        
        evaluator = ModelEvaluator(
            model_path=model_path,
            data_dir=data_dir,
            device=device,
            output_dir=output_dir
        )
        
        print("\n" + "-" * 80)
        print(" å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        # æ‰§è¡Œè¯„ä¼°
        results = evaluator.evaluate_model()
        
        # è¯¦ç»†æ‰“å°ç»“æœ
        print("\n" + "=" * 80)
        print(" è¯„ä¼°ç»“æœè¯¦ç»†æŠ¥å‘Š")
        print("=" * 80)
        
        # Diceç³»æ•°ç»Ÿè®¡
        print(f"\n Diceç³»æ•° (åˆ†å‰²å‡†ç¡®æ€§æŒ‡æ ‡):")
        print(f"  â”œâ”€ å¹³å‡å€¼: {results['mean_dice']:.4f} Â± {results['std_dice']:.4f}")
        print(f"  â”œâ”€ ä¸­ä½æ•°: {results['median_dice']:.4f}")
        print(f"  â”œâ”€ æœ€å°å€¼: {results['min_dice']:.4f}")
        print(f"  â””â”€ æœ€å¤§å€¼: {results['max_dice']:.4f}")
        
        # Hausdorffè·ç¦»ç»Ÿè®¡
        print(f"\n Hausdorffè·ç¦» (è¾¹ç•Œå‡†ç¡®æ€§æŒ‡æ ‡):")
        print(f"  â”œâ”€ å¹³å‡å€¼: {results['mean_hd']:.4f} Â± {results['std_hd']:.4f}")
        print(f"  â””â”€ ä¸­ä½æ•°: {results['median_hd']:.4f}")
        
        # è¡¨é¢è·ç¦»ç»Ÿè®¡
        print(f"\n è¡¨é¢è·ç¦» (è¡¨é¢åŒ¹é…æŒ‡æ ‡):")
        print(f"  â”œâ”€ å¹³å‡å€¼: {results['mean_surface']:.4f} Â± {results['std_surface']:.4f}")
        print(f"  â””â”€ ä¸­ä½æ•°: {results['median_surface']:.4f}")
        
        # è¯„ä¼°æ¡ˆä¾‹ç»Ÿè®¡
        print(f"\n è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  â”œâ”€ æ€»æ¡ˆä¾‹æ•°: {results['total_cases']}")
        print(f"  â”œâ”€ æˆåŠŸè¯„ä¼°: {results['total_cases']}")
        print(f"  â””â”€ å¤±è´¥æ¡ˆä¾‹: 0")
        
        # æ€§èƒ½è¯„çº§
        mean_dice = results['mean_dice']
        if mean_dice >= 0.9:
            performance_level = "[ä¼˜ç§€] (Excellent)"
        elif mean_dice >= 0.8:
            performance_level = "[è‰¯å¥½] (Good)"
        elif mean_dice >= 0.7:
            performance_level = "[ä¸­ç­‰] (Fair)"
        elif mean_dice >= 0.6:
            performance_level = "[ä¸€èˆ¬] (Poor)"
        else:
            performance_level = "[è¾ƒå·®] (Very Poor)"
        
        print(f"\næ¨¡å‹æ€§èƒ½è¯„çº§: {performance_level}")
        
        # è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        print(f"\n è¾“å‡ºæ–‡ä»¶ä¿¡æ¯:")
        print(f"  â”œâ”€ æ¡ˆä¾‹è¯¦ç»†ç»“æœ: {output_dir}/case_results.csv")
        print(f"  â”œâ”€ æ€»ä½“ç»Ÿè®¡æŠ¥å‘Š: {output_dir}/summary_results.txt")
        print(f"  â”œâ”€ ç»“æœåˆ†å¸ƒå›¾: {output_dir}/results_distribution.png")
        print(f"  â””â”€ å¯è§†åŒ–å›¾åƒ: {output_dir}/visualizations/")
        
        print("\n" + "=" * 80)
        print("[æˆåŠŸ] æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        print(f"\n[é”™è¯¯] è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def run_inference(args, device):
    """
    è¿è¡Œæ¨ç†
    """
    try:
        # å‚æ•°éªŒè¯
        if not args.input:
            print("[é”™è¯¯] æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š --input å‚æ•°")
            return False
        
        if not args.output:
            print("[é”™è¯¯] æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š --output å‚æ•°")
            return False
        
        # å¦‚æœæœªæŒ‡å®šè®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼Œå°†æ ¹æ®æ—¶é—´ä¼˜å…ˆåŸåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        if not hasattr(args, 'model_path') or not args.model_path:
            outputs_dir = './outputs'
            if os.path.exists(outputs_dir):
                # æŸ¥æ‰¾outputsç›®å½•ä¸‹çš„æ‰€æœ‰æ¨¡å‹ç›®å½•
                model_dirs = [d for d in os.listdir(outputs_dir) 
                             if os.path.isdir(os.path.join(outputs_dir, d))]
                if model_dirs:
                    # é€‰æ‹©æœ€æ–°çš„æ¨¡å‹ç›®å½•
                    latest_model_dir = max(model_dirs, 
                                         key=lambda x: os.path.getmtime(os.path.join(outputs_dir, x)))
                    args.model_path = os.path.join(outputs_dir, latest_model_dir)
                    print(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°è®­ç»ƒçš„æ¨¡å‹: {args.model_path}")
                else:
                    print(f"[é”™è¯¯] åœ¨ {outputs_dir} ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒå¥½çš„æ¨¡å‹")
                    return False
            else:
                print(f"[é”™è¯¯] è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {outputs_dir}")
                return False
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(args.model_path):
            print(f"[é”™è¯¯] æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            return False
        
        # æ£€æŸ¥è¾“å…¥è·¯å¾„
        if not os.path.exists(args.input):
            print(f"[é”™è¯¯] è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.input}")
            return False
        
        print("\n" + "="*80)
        print("å¼€å§‹æ¨ç†")
        print("="*80)
        print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
        print(f"è¾“å…¥è·¯å¾„: {args.input}")
        print(f"è¾“å‡ºè·¯å¾„: {args.output}")
        print(f"æ¨ç†æ¨¡å¼: {'æ‰¹é‡æ¨ç†' if args.batch_inference else 'å•æ–‡ä»¶æ¨ç†'}")
        
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        inference_engine = InferenceEngine(
            model_path=args.model_path,
            device=device,
            roi_size=tuple(args.roi_size),
            sw_batch_size=args.sw_batch_size,
            overlap=args.overlap
        )
        
        # æ‰§è¡Œæ¨ç†
        if args.batch_inference:
            # æ‰¹é‡æ¨ç†
            results = inference_engine.predict_batch(
                input_dir=args.input,
                output_dir=args.output,
                save_visualization=not args.no_visualization
            )
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if 'error' not in r)
            print(f"\næ¨ç†å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")
            
            if success_count < len(results):
                failed_files = [r['input_path'] for r in results if 'error' in r]
                print(f"å¤±è´¥çš„æ–‡ä»¶: {failed_files}")
        
        else:
            # å•æ–‡ä»¶æ¨ç†
            result = inference_engine.predict_single_case(
                image_path=args.input,
                output_path=args.output,
                save_visualization=not args.no_visualization
            )
            
            print(f"\næ¨ç†å®Œæˆ")
            print(f"è¾“å…¥: {result['input_path']}")
            if 'output_path' in result:
                print(f"è¾“å‡º: {result['output_path']}")
            if 'visualization_path' in result:
                print(f"å¯è§†åŒ–: {result['visualization_path']}")
        
        print("\n" + "="*80)
        print("[æˆåŠŸ] æ¨ç†å®Œæˆï¼")
        print("="*80)
        
        return True
        
    except Exception as e:
        print(f"\n[é”™è¯¯] æ¨ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False





def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(
        description='åŒ»å­¦å›¾åƒåˆ†å‰²é¡¹ç›®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""é»˜è®¤è®­ç»ƒï¼ˆUNetæ¨¡å‹ï¼Œ500è½®ï¼‰"""
    )
    
    parser.add_argument(
        '--mode', 
        type=str, 
        choices=['train', 'evaluate', 'inference'],
        required=True,
        help='è¿è¡Œæ¨¡å¼: trainï¼ˆè®­ç»ƒï¼‰, evaluateï¼ˆè¯„ä¼°ï¼‰, inferenceï¼ˆæ¨ç†ï¼‰'
    )
    

    
    parser.add_argument(
        '--data_dir',
        type=str,
        required=True,
        help='æ•°æ®ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚ï¼špath/to/medical_data/training_data'
    )
    
    parser.add_argument(
        '--dataset_type',
        type=str,
        choices=['BraTS', 'MS_MultiSpine', 'auto'],
        default='auto',
        help='æ•°æ®é›†ç±»å‹ï¼šBraTSï¼ˆè„‘è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ï¼‰ã€MS_MultiSpineï¼ˆå¤šå‘æ€§ç¡¬åŒ–è„ŠæŸ±æ•°æ®é›†ï¼‰æˆ– autoï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰'
    )
    
    parser.add_argument(
        '--model_path',
        type=str,
        default='./outputs/models/',
        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„æˆ–æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆå°†è‡ªåŠ¨æŸ¥æ‰¾å¯¹åº”æ¨¡å‹ç±»å‹ä¸‹çš„best_model.pthï¼‰'
    )
    
    parser.add_argument(
        '--output_dir',
        type=str,
        default='./outputs',
        help='è¾“å‡ºç›®å½•'
    )
    
    parser.add_argument(
        '--batch_size',
        type=int,
        default=None,
        help='æ‰¹æ¬¡å¤§å°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--epochs',
        type=int,
        default=500,
        help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤500è½®ï¼‰'
    )
    
    parser.add_argument(
        '--learning_rate',
        type=float,
        default=None,
        help='å­¦ä¹ ç‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--model_name',
        type=str,
        choices=get_all_supported_models(),
        help='è¦è®­ç»ƒçš„å•ä¸ªæ¨¡å‹åç§°'
    )
    
    parser.add_argument(
        '--model_names',
        type=str,
        nargs='+',
        choices=get_all_supported_models(),
        help='è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨ï¼Œç”¨äºå¤šæ¨¡å‹è®­ç»ƒ'
    )


    
    parser.add_argument(
        '--parallel',
        type=lambda x: x.lower() == 'true',
        default=True,
        help='æ˜¯å¦ä½¿ç”¨å¹¶è¡Œè®­ç»ƒæ¨¡å¼ï¼štrueï¼ˆå¹¶è¡Œè®­ç»ƒï¼Œé»˜è®¤ï¼‰æˆ– falseï¼ˆé€ä¸ªè®­ç»ƒï¼‰'
    )
    
    # æ¨¡å‹ç±»åˆ«å‚æ•°ï¼ˆå¿…éœ€ï¼‰
    parser.add_argument(
        '--model_category',
        type=str,
        choices=['basic', 'advanced'],
        required=True,
        help='æ¨¡å‹ç±»åˆ«ï¼šbasicï¼ˆåŸºç¡€æ¨¡å‹,éœ€æŒ‡å®š--model_nameæˆ–--model_namesï¼‰æˆ– advancedï¼ˆå¤åˆæ¶æ„æ¨¡å‹,éœ€æŒ‡å®š--model_typeï¼‰'
    )
    
    parser.add_argument(
        '--model_type',
        type=str,
        choices=['fusion', 'distillation', 'nas', 'nas_distillation'],
        default='fusion',
        help='å¤åˆæ¶æ„æ¨¡å‹ç±»å‹ï¼šfusionï¼ˆèåˆç½‘ç»œï¼‰ã€distillationï¼ˆçŸ¥è¯†è’¸é¦ï¼‰ã€nasï¼ˆç¥ç»æ¶æ„æœç´¢ï¼‰ã€nas_distillationï¼ˆNAS-è’¸é¦é›†æˆï¼‰'
    )
    
    parser.add_argument(
        '--teacher_models',
        type=str,
        nargs='+',
        choices=get_all_supported_models(),
        help='çŸ¥è¯†è’¸é¦çš„æ•™å¸ˆæ¨¡å‹åˆ—è¡¨'
    )
    
    parser.add_argument(
        '--student_model',
        type=str,
        choices=get_all_supported_models(),
        default='VNet3D',
        help='çŸ¥è¯†è’¸é¦çš„å­¦ç”Ÿæ¨¡å‹'
    )
    
    parser.add_argument(
        '--fusion_models',
        type=str,
        nargs='+',
        choices=get_all_supported_models(),
        help='èåˆç½‘ç»œçš„åŸºç¡€æ¨¡å‹åˆ—è¡¨'
    )
    
    parser.add_argument(
        '--fusion_type',
        type=str,
        choices=['cross_attention', 'channel_attention', 'spatial_attention', 'adaptive'],
        default='cross_attention',
        help='èåˆç±»å‹ï¼šcross_attention/channel_attention/spatial_attention/adaptive'
    )
    
    parser.add_argument(
        '--fusion_channels',
        type=int,
        nargs='+',
        default=[64, 128, 256, 512],
        help='èåˆç½‘ç»œé€šé“é…ç½®'
    )
    
    parser.add_argument(
        '--distillation_temperature',
        type=float,
        default=4.0,
        help='çŸ¥è¯†è’¸é¦æ¸©åº¦å‚æ•°'
    )
    
    parser.add_argument(
        '--distillation_alpha',
        type=float,
        default=0.7,
        help='çŸ¥è¯†è’¸é¦è½¯æ ‡ç­¾æƒé‡'
    )
    
    # é¢„è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument(
        '--pretrained_dir',
        type=str,
        default='./pretrained_teachers',
        help='é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--pretrain_teachers',
        type=bool,
        default=True,
        help='å¯ç”¨æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒï¼ˆé»˜è®¤å¯ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--teacher_epochs',
        type=int,
        default=100,
        help='æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒè½®æ•°'
    )
    
    parser.add_argument(
        '--force_retrain_teachers',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°è®­ç»ƒå·²å­˜åœ¨çš„é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹'
    )
    
    parser.add_argument(
        '--nas_epochs',
        type=int,
        default=50,
        help='NASæœç´¢è½®æ•°'
    )
    
    # NAS-è’¸é¦é›†æˆç›¸å…³å‚æ•°
    parser.add_argument(
        '--distillation_epochs',
        type=int,
        default=100,
        help='NAS-è’¸é¦é›†æˆæ¨¡å¼ï¼šçŸ¥è¯†è’¸é¦è®­ç»ƒè½®æ•°'
    )
    
    parser.add_argument(
        '--distillation_lr',
        type=float,
        default=1e-4,
        help='NAS-è’¸é¦é›†æˆæ¨¡å¼ï¼šçŸ¥è¯†è’¸é¦é˜¶æ®µå­¦ä¹ ç‡'
    )
    
    parser.add_argument(
        '--nas_distillation_save_dir',
        type=str,
        default='./checkpoints/nas_distillation',
        help='NAS-è’¸é¦é›†æˆæ¨¡å¼ï¼šæ¨¡å‹ä¿å­˜ç›®å½•'
    )
    
    # NASç›¸å…³å‚æ•°
    parser.add_argument(
        '--nas_type',
        type=str,
        choices=['searcher', 'progressive', 'supernet'],
        default='supernet',
        help='NASæœç´¢ç­–ç•¥ç±»å‹ï¼šsupernetï¼ˆè¶…ç½‘ç»œè®­ç»ƒï¼‰ã€searcherï¼ˆDARTSå¯å¾®åˆ†æ¶æ„æœç´¢ï¼‰ã€progressiveï¼ˆæ¸è¿›å¼æœç´¢ï¼‰'
    )
    
    #è¶…ç½‘ç»œå‚æ•°ï¼ˆnas_type=supernetæ—¶/ä¸æŒ‡å®šä½¿ç”¨é»˜è®¤å€¼æ—¶ä½¿ç”¨ï¼‰
    parser.add_argument(
        '--base_channels',
        type=int,
        default=32,
        help='NASç½‘ç»œåŸºç¡€é€šé“æ•°ï¼ˆé»˜è®¤32ï¼Œæ¨è16-64ä¹‹é—´ï¼‰'
    )
    
    parser.add_argument(
        '--num_layers',
        type=int,
        default=4,
        help='NASç½‘ç»œå±‚æ•°ï¼ˆé»˜è®¤4ï¼Œæ¨è3-6å±‚ä¹‹é—´ï¼‰'
    )
    
    # DARTSæœç´¢å‚æ•°ï¼ˆnas_type=searcheræ—¶ä½¿ç”¨ï¼‰
    parser.add_argument(
        '--arch_lr',
        type=float,
        default=3e-4,
        help='æ¶æ„å‚æ•°å­¦ä¹ ç‡ï¼ˆé»˜è®¤3e-4ï¼Œæ¨è1e-4åˆ°5e-4ä¹‹é—´ï¼‰'
    )
    
    parser.add_argument(
        '--model_lr',
        type=float,
        default=1e-3,
        help='æ¨¡å‹æƒé‡å­¦ä¹ ç‡ï¼ˆé»˜è®¤1e-3ï¼Œæ¨è5e-4åˆ°2e-3ä¹‹é—´ï¼‰'
    )
    
    # æ¸è¿›å¼NASå‚æ•°ï¼ˆnas_type=progressiveæ—¶ä½¿ç”¨ï¼‰
    parser.add_argument(
        '--max_layers',
        type=int,
        default=8,
        help='æœ€å¤§ç½‘ç»œå±‚æ•°ï¼ˆé»˜è®¤8ï¼Œæ¨è4-10å±‚ä¹‹é—´ï¼‰'
    )
    
    parser.add_argument(
        '--start_layers',
        type=int,
        default=2,
        help='èµ·å§‹ç½‘ç»œå±‚æ•°ï¼ˆé»˜è®¤2ï¼Œæ¨è2-4å±‚å¼€å§‹ï¼‰'
    )

    
    parser.add_argument(
        '--auto_adjust',
        action='store_true',
        default=True,
        help='æ˜¯å¦æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒèŠ‚å‚æ•°'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        choices=['cpu', 'cuda', 'auto'],
        default='auto',
        help='è®¡ç®—è®¾å¤‡é€‰æ‹©: cpu, cuda, autoï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰'
    )
    
    # æ¨ç†æ¨¡å¼ä¸“ç”¨å‚æ•°
    parser.add_argument(
        '--input',
        type=str,
        help='æ¨ç†æ¨¡å¼ï¼šè¾“å…¥æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        help='æ¨ç†æ¨¡å¼ï¼šè¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--batch_inference',
        action='store_true',
        help='æ¨ç†æ¨¡å¼ï¼šå¯ç”¨æ‰¹é‡æ¨ç†'
    )
    
    parser.add_argument(
        '--roi_size',
        type=int,
        nargs=3,
        default=[128, 128, 128],
        help='æ¨ç†æ¨¡å¼ï¼šæ»‘åŠ¨çª—å£å¤§å°'
    )
    
    parser.add_argument(
        '--sw_batch_size',
        type=int,
        default=4,
        help='æ¨ç†æ¨¡å¼ï¼šæ»‘åŠ¨çª—å£æ‰¹æ¬¡å¤§å°'
    )
    
    parser.add_argument(
        '--overlap',
        type=float,
        default=0.6,
        help='æ¨ç†æ¨¡å¼ï¼šæ»‘åŠ¨çª—å£é‡å ç‡'
    )
    
    parser.add_argument(
        '--no_visualization',
        action='store_true',
        help='æ¨ç†æ¨¡å¼ï¼šä¸ä¿å­˜å¯è§†åŒ–ç»“æœ'
    )

    
    args = parser.parse_args()
    
    # å¤„ç†æ¨¡å‹å‚æ•°é€»è¾‘
    if args.model_category == 'basic':
        # åŸºç¡€æ¨¡å‹è®­ç»ƒå‚æ•°éªŒè¯
        if args.model_name and args.model_names:
            print("[é”™è¯¯] ä¸èƒ½åŒæ—¶æŒ‡å®š --model_name å’Œ --model_names å‚æ•°")
            return 1
        elif args.model_name:
            # å•ä¸ªåŸºç¡€æ¨¡å‹è®­ç»ƒ
            args.model_names = [args.model_name]
        elif args.model_names:
            # å¤šä¸ªåŸºç¡€æ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹åˆ—è¡¨
            pass
        else:
            print("[é”™è¯¯] åŸºç¡€æ¨¡å‹è®­ç»ƒå¿…é¡»æŒ‡å®š --model_name æˆ– --model_names å‚æ•°")
            return 1
    elif args.model_category == 'advanced':
        # å¤åˆæ¶æ„æ¨¡å‹è®­ç»ƒå‚æ•°éªŒè¯
        if not args.model_type:
            print("[é”™è¯¯] å¤åˆæ¶æ„æ¨¡å‹è®­ç»ƒå¿…é¡»æŒ‡å®šæœ‰æ•ˆçš„ --model_type å‚æ•° (fusion/distillation/nas)")
            return 1
        # å¤åˆæ¶æ„æ¨¡å‹ä¸ä½¿ç”¨ model_names
        args.model_names = []
    
    # å¤„ç†æ•™å¸ˆæ¨¡å‹é»˜è®¤é€‰æ‹©é€»è¾‘
    if args.model_type == 'distillation' and not args.teacher_models:
        # çŸ¥è¯†è’¸é¦æ¨¡å¼ä¸‹ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šæ•™å¸ˆæ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰åŸºç¡€æ¨¡å‹
        args.teacher_models = get_all_supported_models()
        print(f"[ä¿¡æ¯] çŸ¥è¯†è’¸é¦æ¨¡å¼ï¼šè‡ªåŠ¨é€‰æ‹©æ‰€æœ‰åŸºç¡€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹: {args.teacher_models}")
    
    # å¤„ç†NAS-è’¸é¦é›†æˆæ¨¡å¼çš„æ•™å¸ˆæ¨¡å‹é»˜è®¤é€‰æ‹©é€»è¾‘
    if args.model_type == 'nas_distillation' and not args.teacher_models:
        # NAS-è’¸é¦é›†æˆæ¨¡å¼ä¸‹ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šæ•™å¸ˆæ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰åŸºç¡€æ¨¡å‹
        args.teacher_models = get_all_supported_models()
        print(f"[ä¿¡æ¯] NAS-è’¸é¦é›†æˆæ¨¡å¼ï¼šè‡ªåŠ¨é€‰æ‹©æ‰€æœ‰åŸºç¡€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹: {args.teacher_models}")
    
    # å¤„ç†æ¨¡å‹è®­ç»ƒé€»è¾‘
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œé»˜è®¤è®­ç»ƒå•ä¸ªVNet3Dæ¨¡å‹
    if not args.model_names:
        args.model_names = ['VNet3D']  # é»˜è®¤å•æ¨¡å‹è®­ç»ƒ
    
    # è®¾å¤‡é…ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        device_name = 'GPU' if torch.cuda.is_available() else 'CPU'
    elif args.device == 'cuda':
        if torch.cuda.is_available():
            device = torch.device('cuda')
            device_name = 'GPU'
        else:
            print("[è­¦å‘Š] æŒ‡å®šä½¿ç”¨GPUä½†CUDAä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPU")
            device = torch.device('cpu')
            device_name = 'CPU'
    else:  # cpu
        device = torch.device('cpu')
        device_name = 'CPU'
    
    # è®¾ç½®å…¨å±€è®¾å¤‡
    if device.type == 'cuda':
        torch.cuda.set_device(device.index if device.index is not None else 0)
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 80)
    print("åŸºäºMONAIæ¡†æ¶çš„åŒ»å­¦å›¾åƒåˆ†å‰²è§£å†³æ–¹æ¡ˆ")
    print("=" * 80)
    print(f"è®¡ç®—è®¾å¤‡: {device_name} ({device})")
    if device.type == 'cuda':
        print(f"GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    if args.mode != 'inference':
        print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    if args.mode in ['evaluate', 'inference']:
        print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    if args.mode == 'inference':
        print(f"è¾“å…¥è·¯å¾„: {args.input}")
        print(f"è¾“å‡ºè·¯å¾„: {args.output}")
    else:
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•ï¼ˆæ¨ç†æ¨¡å¼ä¸éœ€è¦ï¼‰
    if args.mode != 'inference' and not os.path.exists(args.data_dir):
        print(f"\n[é”™è¯¯] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        print("è¯·æ£€æŸ¥æ•°æ®è·¯å¾„æˆ–ä¸‹è½½æ•°æ®é›†")
        return 1
    
    success = True
    
    try:
        if args.mode == 'train':
            success = run_simplified_training(args, device)
            
        elif args.mode == 'evaluate':
            success = run_evaluation(args.model_path, args.data_dir, args.output_dir + '/evaluation', device)
            
        elif args.mode == 'inference':
            success = run_inference(args, device)
    
    except KeyboardInterrupt:
        print("\n\n[è­¦å‘Š] ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 1
    except Exception as e:
        print(f"\n\n[é”™è¯¯] æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    if success:
        print("\n" + "=" * 80)
        print("[æˆåŠŸ] æ‰§è¡Œå®Œæˆï¼")
        print("=" * 80)
        return 0
    else:
        print("\n" + "=" * 80)
        print("[å¤±è´¥] æ‰§è¡Œå¤±è´¥ï¼")
        print("=" * 80)
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)